{"docstore/metadata": {"82d84f09-7d30-435f-839f-bf83e19b907b": {"doc_hash": "b75d4ee1f34ec4f633bd750cdc013c8465514c53ed5e8a2c488c15f028158ec9"}, "d1941f75-57a9-44d4-a7f6-60725d8b7092": {"doc_hash": "65aec831cf9636e74412e849d83dbff0a466661fc14f30eae78dbc0cf3c43ea7", "ref_doc_id": "82d84f09-7d30-435f-839f-bf83e19b907b"}, "d89a01ad-5a21-4fa4-8821-d76bcd957d1c": {"doc_hash": "a25a0eb2a0126276c4be95fc5f9491f64d2043606d10243954631b5a78a8b4cb", "ref_doc_id": "82d84f09-7d30-435f-839f-bf83e19b907b"}, "f8ee740d-cd33-4327-8880-b2e6157ff358": {"doc_hash": "0f2d255ed9d6cb91b7653130843a854a732b704c81d83cc4c09926f2bc63b953", "ref_doc_id": "82d84f09-7d30-435f-839f-bf83e19b907b"}, "013b8180-fc65-41e2-a304-a79ad1e6dbbe": {"doc_hash": "bf658989f0320032b00ac1c0606e66bf6f0d44f55c70a044f250c5c2f495867e", "ref_doc_id": "82d84f09-7d30-435f-839f-bf83e19b907b"}, "73bc6480-4e8d-4faa-b739-34334eac9e0f": {"doc_hash": "3acd3ca3b320e8e62bae733624761ffe3470fdc4b67c0f3c26ceb919bfa45f6b", "ref_doc_id": "82d84f09-7d30-435f-839f-bf83e19b907b"}, "877f6fa0-bc6d-4f2a-b190-1c3fddff9d25": {"doc_hash": "45a0a44dd8f6e1b72951c6fdabb0d4957536c55d1b05b14e5ca853161bf45de1", "ref_doc_id": "82d84f09-7d30-435f-839f-bf83e19b907b"}, "d61d989a-cbfa-435a-a750-cfffed889895": {"doc_hash": "7ccaff5857faa64d0a9823f702b7998f6089e0ada634643df49a943c9b79040f", "ref_doc_id": "82d84f09-7d30-435f-839f-bf83e19b907b"}, "3fa3a56e-1188-41c5-8c58-c43d303f8539": {"doc_hash": "27823941240b82091d25ca3d2cf3d227c16e20160e50d0f33f4beef4f0f70f9f", "ref_doc_id": "82d84f09-7d30-435f-839f-bf83e19b907b"}, "7216234e-c894-44bc-99d3-5f8db881c192": {"doc_hash": "19106c6b5985f02e285a3cd09d593c07408057234b17875097fcb5eef2eeeb1e", "ref_doc_id": "82d84f09-7d30-435f-839f-bf83e19b907b"}, "807d0e15-d9b4-4d49-b92a-53b51b9f7ba6": {"doc_hash": "5ccb500f7f13f6146fb4647d429be0f0eb26fb6f9cf2014d8d9717f705c55793", "ref_doc_id": "82d84f09-7d30-435f-839f-bf83e19b907b"}, "9001860d-70bd-435a-9d19-9be6a541e917": {"doc_hash": "20648bf5af2d99d9a510eb7bff23c169637fdffeba58e071808948d477858357", "ref_doc_id": "82d84f09-7d30-435f-839f-bf83e19b907b"}, "0afe309c-44d9-40db-a58e-530a002c8b55": {"doc_hash": "b9b48789b175b7ab6ebd47f933e41f77b40b7d9d5f4981778a56909df20a8754", "ref_doc_id": "82d84f09-7d30-435f-839f-bf83e19b907b"}, "79b1ee48-d786-4146-a49c-c444f5487301": {"doc_hash": "5086d7e453d88fc57c2a24214f791e52582025903b56487f9b852195bad8fbb1", "ref_doc_id": "82d84f09-7d30-435f-839f-bf83e19b907b"}, "37a49867-b891-4ec4-9a6f-0c3adb4b379b": {"doc_hash": "b2cb5bae61e89e1e1156b1a0bfb4ada0a52e92cdce1ed45ba4bcc5bcd0b36848", "ref_doc_id": "82d84f09-7d30-435f-839f-bf83e19b907b"}, "51895717-bb4a-42ad-8db9-7f2438109c5b": {"doc_hash": "35542a4a922998662eca7a6efb095b37b3adc9fe9a48427fa2f5a3a70f10139c", "ref_doc_id": "82d84f09-7d30-435f-839f-bf83e19b907b"}, "5fa511f5-e4bc-4f0c-ae6d-51ba33671408": {"doc_hash": "33be1540aeedb8de4834af8ea6ff630cd0e5fd352b4b8064c792dd4bba52282d", "ref_doc_id": "82d84f09-7d30-435f-839f-bf83e19b907b"}, "36ddacfe-da6f-4060-b2c5-59200a90ef73": {"doc_hash": "3bff012bedbbbb75f21e0f0f09890eb5bc3b7b409f24f11edbdd1eb9677b0554", "ref_doc_id": "82d84f09-7d30-435f-839f-bf83e19b907b"}, "b0f8925e-9d7a-44de-b385-ce1f0063ddc5": {"doc_hash": "527d357dc62747895599160a29b54a359c234442300859774a3004704e9e11b4", "ref_doc_id": "82d84f09-7d30-435f-839f-bf83e19b907b"}, "08123b7d-7164-4e6b-b9d4-d80b73fd7272": {"doc_hash": "1b981e305102bdf0eaa900c64478e30a2e2a8d09471d9269ae07749d8a4f99ac", "ref_doc_id": "82d84f09-7d30-435f-839f-bf83e19b907b"}, "6feb1a13-9b49-4c67-820a-f7e502d49aa1": {"doc_hash": "a808262ac6fb3c76a7ada3489d4667822fe75401dd1939358c09b5010378ba3f", "ref_doc_id": "82d84f09-7d30-435f-839f-bf83e19b907b"}, "4a978267-d802-40cf-98d3-04cfaef37695": {"doc_hash": "d0685c0b62e6421de77102f9f04f6c9fbfb4cd923e4df4f2b81862bf5bb98d70", "ref_doc_id": "82d84f09-7d30-435f-839f-bf83e19b907b"}, "1c7c8928-6622-45f1-b2c4-02d5e5e0999c": {"doc_hash": "c3a3387d9f9320f18350da6a6f8ab3d88152b50d7c5bc2e7a27e28f84a7afef7", "ref_doc_id": "82d84f09-7d30-435f-839f-bf83e19b907b"}, "308937ff-6ad0-4cbb-96a9-35b8a962caa0": {"doc_hash": "b4bed570ec8a2dde770bbec0b279045c50200d9c6cb6c0edee006e050245f4de", "ref_doc_id": "82d84f09-7d30-435f-839f-bf83e19b907b"}, "da7b9481-f7d5-4857-a91b-963cc0480ddf": {"doc_hash": "c9e18d0fdb1df70c017195270c0531364730cb99d7927e02baae495f42e97874", "ref_doc_id": "82d84f09-7d30-435f-839f-bf83e19b907b"}, "3cf3ca8e-3b4c-4beb-9ff2-e80403c73a90": {"doc_hash": "f31a5d3be96829b90ddae0d048b3e33f16fa21b0e9077f8dc4ffb9718ebb7de1", "ref_doc_id": "82d84f09-7d30-435f-839f-bf83e19b907b"}, "1022e526-1a26-4c14-96c4-e2c296117622": {"doc_hash": "9f4aa4be89cf852809bf7818bd402794ca962b3fb8607ac0cda5d3f3195d4061", "ref_doc_id": "82d84f09-7d30-435f-839f-bf83e19b907b"}, "c442f7c4-8d42-44e4-b7ff-34afbbbb6253": {"doc_hash": "1e6a4bed6b857f626c77be1c22ab868ee09944fc33ef2e7ccf5806f21ba75764", "ref_doc_id": "82d84f09-7d30-435f-839f-bf83e19b907b"}, "69f6d15b-04f8-47fb-8707-2c20e0ae380b": {"doc_hash": "2fef1c37d2c5520b161b4d117a8e4938f701381f9ce36359b043a7ff425cfec9", "ref_doc_id": "82d84f09-7d30-435f-839f-bf83e19b907b"}, "d597229d-1765-4679-81bc-808fae99d8d7": {"doc_hash": "4ef22799516388a66eba69e46efa3567a7f91d00d6c2ecd71f5a700923321317", "ref_doc_id": "82d84f09-7d30-435f-839f-bf83e19b907b"}}, "docstore/data": {"d1941f75-57a9-44d4-a7f6-60725d8b7092": {"__data__": {"id_": "d1941f75-57a9-44d4-a7f6-60725d8b7092", "embedding": null, "metadata": {"window": "Josekutty Jose\nMachine Learning Engineer\n\u2642phone+91 9745949352 /envel\u2322pejosekuttyjose08@gmail.com /linkedinLinkedIn /githubGitHub DagsHub\u1f517LeetCode\nPro\ufb01le Summary\nHighly motivated Machine Learning Engineer over 6+ months of experience in Generative AI with a passion for building\nimpactful AI solutions.  Proven ability to design, develop, and deploy machine learning models across various domains.\n Skilled in data preprocessing, feature engineering, model selection, and hyperparameter tuning .  Experience working with\nTensorFlow, PyTorch, and scikit-learn and Expertise in Quantization, Fine -tuning and improving the models performance.\n", "original_text": "Josekutty Jose\nMachine Learning Engineer\n\u2642phone+91 9745949352 /envel\u2322pejosekuttyjose08@gmail.com /linkedinLinkedIn /githubGitHub DagsHub\u1f517LeetCode\nPro\ufb01le Summary\nHighly motivated Machine Learning Engineer over 6+ months of experience in Generative AI with a passion for building\nimpactful AI solutions. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "82d84f09-7d30-435f-839f-bf83e19b907b", "node_type": "4", "metadata": {}, "hash": "b75d4ee1f34ec4f633bd750cdc013c8465514c53ed5e8a2c488c15f028158ec9", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d89a01ad-5a21-4fa4-8821-d76bcd957d1c", "node_type": "1", "metadata": {"window": "Josekutty Jose\nMachine Learning Engineer\n\u2642phone+91 9745949352 /envel\u2322pejosekuttyjose08@gmail.com /linkedinLinkedIn /githubGitHub DagsHub\u1f517LeetCode\nPro\ufb01le Summary\nHighly motivated Machine Learning Engineer over 6+ months of experience in Generative AI with a passion for building\nimpactful AI solutions.  Proven ability to design, develop, and deploy machine learning models across various domains.\n Skilled in data preprocessing, feature engineering, model selection, and hyperparameter tuning .  Experience working with\nTensorFlow, PyTorch, and scikit-learn and Expertise in Quantization, Fine -tuning and improving the models performance.\n Eager to contribute to a team environment and continuously learn new technologies.\n", "original_text": "Proven ability to design, develop, and deploy machine learning models across various domains.\n"}, "hash": "a25a0eb2a0126276c4be95fc5f9491f64d2043606d10243954631b5a78a8b4cb", "class_name": "RelatedNodeInfo"}}, "text": "Josekutty Jose\nMachine Learning Engineer\n\u2642phone+91 9745949352 /envel\u2322pejosekuttyjose08@gmail.com /linkedinLinkedIn /githubGitHub DagsHub\u1f517LeetCode\nPro\ufb01le Summary\nHighly motivated Machine Learning Engineer over 6+ months of experience in Generative AI with a passion for building\nimpactful AI solutions. ", "start_char_idx": 0, "end_char_idx": 303, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d89a01ad-5a21-4fa4-8821-d76bcd957d1c": {"__data__": {"id_": "d89a01ad-5a21-4fa4-8821-d76bcd957d1c", "embedding": null, "metadata": {"window": "Josekutty Jose\nMachine Learning Engineer\n\u2642phone+91 9745949352 /envel\u2322pejosekuttyjose08@gmail.com /linkedinLinkedIn /githubGitHub DagsHub\u1f517LeetCode\nPro\ufb01le Summary\nHighly motivated Machine Learning Engineer over 6+ months of experience in Generative AI with a passion for building\nimpactful AI solutions.  Proven ability to design, develop, and deploy machine learning models across various domains.\n Skilled in data preprocessing, feature engineering, model selection, and hyperparameter tuning .  Experience working with\nTensorFlow, PyTorch, and scikit-learn and Expertise in Quantization, Fine -tuning and improving the models performance.\n Eager to contribute to a team environment and continuously learn new technologies.\n", "original_text": "Proven ability to design, develop, and deploy machine learning models across various domains.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "82d84f09-7d30-435f-839f-bf83e19b907b", "node_type": "4", "metadata": {}, "hash": "b75d4ee1f34ec4f633bd750cdc013c8465514c53ed5e8a2c488c15f028158ec9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d1941f75-57a9-44d4-a7f6-60725d8b7092", "node_type": "1", "metadata": {"window": "Josekutty Jose\nMachine Learning Engineer\n\u2642phone+91 9745949352 /envel\u2322pejosekuttyjose08@gmail.com /linkedinLinkedIn /githubGitHub DagsHub\u1f517LeetCode\nPro\ufb01le Summary\nHighly motivated Machine Learning Engineer over 6+ months of experience in Generative AI with a passion for building\nimpactful AI solutions.  Proven ability to design, develop, and deploy machine learning models across various domains.\n Skilled in data preprocessing, feature engineering, model selection, and hyperparameter tuning .  Experience working with\nTensorFlow, PyTorch, and scikit-learn and Expertise in Quantization, Fine -tuning and improving the models performance.\n", "original_text": "Josekutty Jose\nMachine Learning Engineer\n\u2642phone+91 9745949352 /envel\u2322pejosekuttyjose08@gmail.com /linkedinLinkedIn /githubGitHub DagsHub\u1f517LeetCode\nPro\ufb01le Summary\nHighly motivated Machine Learning Engineer over 6+ months of experience in Generative AI with a passion for building\nimpactful AI solutions. "}, "hash": "65aec831cf9636e74412e849d83dbff0a466661fc14f30eae78dbc0cf3c43ea7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f8ee740d-cd33-4327-8880-b2e6157ff358", "node_type": "1", "metadata": {"window": "Josekutty Jose\nMachine Learning Engineer\n\u2642phone+91 9745949352 /envel\u2322pejosekuttyjose08@gmail.com /linkedinLinkedIn /githubGitHub DagsHub\u1f517LeetCode\nPro\ufb01le Summary\nHighly motivated Machine Learning Engineer over 6+ months of experience in Generative AI with a passion for building\nimpactful AI solutions.  Proven ability to design, develop, and deploy machine learning models across various domains.\n Skilled in data preprocessing, feature engineering, model selection, and hyperparameter tuning .  Experience working with\nTensorFlow, PyTorch, and scikit-learn and Expertise in Quantization, Fine -tuning and improving the models performance.\n Eager to contribute to a team environment and continuously learn new technologies.\n Technical Skills\nLanguages : Python, HTML, SQL, CSS, Neo4j, Cypher\nFrameworks : Tensor\ufb02ow, Langchain, Streamlit, Pytorch , Fastapi\nLibraries : Excel, Pandas, Numpy, Scipy, Scikit-learn\nTools & Platforms : Git, Dagshub,\nData Visualization : Power BI, Tableau , Matplotlib, Seaborn\nDomain Skills : NLP, Statistical Modeling, Predictive Modeling, Data Analysis, Transformers, Machine Learning, Web\nScraping, Generative AI, Deep Learning, CNN,Computer Vision, Arti\ufb01cial General Intelligence, AI\nProjects\nDetecting Air pollution in Mongolia using NLP |Apify, T5 Bert , LSTM, Deberta LLM, Hugging Face DagsHub\n\u2022Collaborated with a 23- member team to develop and implement Environmental Sentinel on Social Media using\nsentiment analysisf or real-time air pollution monitoring in Mongolia.\n", "original_text": "Skilled in data preprocessing, feature engineering, model selection, and hyperparameter tuning . "}, "hash": "0f2d255ed9d6cb91b7653130843a854a732b704c81d83cc4c09926f2bc63b953", "class_name": "RelatedNodeInfo"}}, "text": "Proven ability to design, develop, and deploy machine learning models across various domains.\n", "start_char_idx": 303, "end_char_idx": 397, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f8ee740d-cd33-4327-8880-b2e6157ff358": {"__data__": {"id_": "f8ee740d-cd33-4327-8880-b2e6157ff358", "embedding": null, "metadata": {"window": "Josekutty Jose\nMachine Learning Engineer\n\u2642phone+91 9745949352 /envel\u2322pejosekuttyjose08@gmail.com /linkedinLinkedIn /githubGitHub DagsHub\u1f517LeetCode\nPro\ufb01le Summary\nHighly motivated Machine Learning Engineer over 6+ months of experience in Generative AI with a passion for building\nimpactful AI solutions.  Proven ability to design, develop, and deploy machine learning models across various domains.\n Skilled in data preprocessing, feature engineering, model selection, and hyperparameter tuning .  Experience working with\nTensorFlow, PyTorch, and scikit-learn and Expertise in Quantization, Fine -tuning and improving the models performance.\n Eager to contribute to a team environment and continuously learn new technologies.\n Technical Skills\nLanguages : Python, HTML, SQL, CSS, Neo4j, Cypher\nFrameworks : Tensor\ufb02ow, Langchain, Streamlit, Pytorch , Fastapi\nLibraries : Excel, Pandas, Numpy, Scipy, Scikit-learn\nTools & Platforms : Git, Dagshub,\nData Visualization : Power BI, Tableau , Matplotlib, Seaborn\nDomain Skills : NLP, Statistical Modeling, Predictive Modeling, Data Analysis, Transformers, Machine Learning, Web\nScraping, Generative AI, Deep Learning, CNN,Computer Vision, Arti\ufb01cial General Intelligence, AI\nProjects\nDetecting Air pollution in Mongolia using NLP |Apify, T5 Bert , LSTM, Deberta LLM, Hugging Face DagsHub\n\u2022Collaborated with a 23- member team to develop and implement Environmental Sentinel on Social Media using\nsentiment analysisf or real-time air pollution monitoring in Mongolia.\n", "original_text": "Skilled in data preprocessing, feature engineering, model selection, and hyperparameter tuning . "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "82d84f09-7d30-435f-839f-bf83e19b907b", "node_type": "4", "metadata": {}, "hash": "b75d4ee1f34ec4f633bd750cdc013c8465514c53ed5e8a2c488c15f028158ec9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d89a01ad-5a21-4fa4-8821-d76bcd957d1c", "node_type": "1", "metadata": {"window": "Josekutty Jose\nMachine Learning Engineer\n\u2642phone+91 9745949352 /envel\u2322pejosekuttyjose08@gmail.com /linkedinLinkedIn /githubGitHub DagsHub\u1f517LeetCode\nPro\ufb01le Summary\nHighly motivated Machine Learning Engineer over 6+ months of experience in Generative AI with a passion for building\nimpactful AI solutions.  Proven ability to design, develop, and deploy machine learning models across various domains.\n Skilled in data preprocessing, feature engineering, model selection, and hyperparameter tuning .  Experience working with\nTensorFlow, PyTorch, and scikit-learn and Expertise in Quantization, Fine -tuning and improving the models performance.\n Eager to contribute to a team environment and continuously learn new technologies.\n", "original_text": "Proven ability to design, develop, and deploy machine learning models across various domains.\n"}, "hash": "a25a0eb2a0126276c4be95fc5f9491f64d2043606d10243954631b5a78a8b4cb", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "013b8180-fc65-41e2-a304-a79ad1e6dbbe", "node_type": "1", "metadata": {"window": "Josekutty Jose\nMachine Learning Engineer\n\u2642phone+91 9745949352 /envel\u2322pejosekuttyjose08@gmail.com /linkedinLinkedIn /githubGitHub DagsHub\u1f517LeetCode\nPro\ufb01le Summary\nHighly motivated Machine Learning Engineer over 6+ months of experience in Generative AI with a passion for building\nimpactful AI solutions.  Proven ability to design, develop, and deploy machine learning models across various domains.\n Skilled in data preprocessing, feature engineering, model selection, and hyperparameter tuning .  Experience working with\nTensorFlow, PyTorch, and scikit-learn and Expertise in Quantization, Fine -tuning and improving the models performance.\n Eager to contribute to a team environment and continuously learn new technologies.\n Technical Skills\nLanguages : Python, HTML, SQL, CSS, Neo4j, Cypher\nFrameworks : Tensor\ufb02ow, Langchain, Streamlit, Pytorch , Fastapi\nLibraries : Excel, Pandas, Numpy, Scipy, Scikit-learn\nTools & Platforms : Git, Dagshub,\nData Visualization : Power BI, Tableau , Matplotlib, Seaborn\nDomain Skills : NLP, Statistical Modeling, Predictive Modeling, Data Analysis, Transformers, Machine Learning, Web\nScraping, Generative AI, Deep Learning, CNN,Computer Vision, Arti\ufb01cial General Intelligence, AI\nProjects\nDetecting Air pollution in Mongolia using NLP |Apify, T5 Bert , LSTM, Deberta LLM, Hugging Face DagsHub\n\u2022Collaborated with a 23- member team to develop and implement Environmental Sentinel on Social Media using\nsentiment analysisf or real-time air pollution monitoring in Mongolia.\n \u2022Applied 3 key techniques: Market Research ,Data Collection , and Preprocessing , leading to successful Model\nDevelopment\n\u2022Successfully scraped and collected 75,469 pieces of Mongolian text from diverse sources, including Mongolian social\nmedia platforms and news channels.\n", "original_text": "Experience working with\nTensorFlow, PyTorch, and scikit-learn and Expertise in Quantization, Fine -tuning and improving the models performance.\n"}, "hash": "bf658989f0320032b00ac1c0606e66bf6f0d44f55c70a044f250c5c2f495867e", "class_name": "RelatedNodeInfo"}}, "text": "Skilled in data preprocessing, feature engineering, model selection, and hyperparameter tuning . ", "start_char_idx": 397, "end_char_idx": 494, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "013b8180-fc65-41e2-a304-a79ad1e6dbbe": {"__data__": {"id_": "013b8180-fc65-41e2-a304-a79ad1e6dbbe", "embedding": null, "metadata": {"window": "Josekutty Jose\nMachine Learning Engineer\n\u2642phone+91 9745949352 /envel\u2322pejosekuttyjose08@gmail.com /linkedinLinkedIn /githubGitHub DagsHub\u1f517LeetCode\nPro\ufb01le Summary\nHighly motivated Machine Learning Engineer over 6+ months of experience in Generative AI with a passion for building\nimpactful AI solutions.  Proven ability to design, develop, and deploy machine learning models across various domains.\n Skilled in data preprocessing, feature engineering, model selection, and hyperparameter tuning .  Experience working with\nTensorFlow, PyTorch, and scikit-learn and Expertise in Quantization, Fine -tuning and improving the models performance.\n Eager to contribute to a team environment and continuously learn new technologies.\n Technical Skills\nLanguages : Python, HTML, SQL, CSS, Neo4j, Cypher\nFrameworks : Tensor\ufb02ow, Langchain, Streamlit, Pytorch , Fastapi\nLibraries : Excel, Pandas, Numpy, Scipy, Scikit-learn\nTools & Platforms : Git, Dagshub,\nData Visualization : Power BI, Tableau , Matplotlib, Seaborn\nDomain Skills : NLP, Statistical Modeling, Predictive Modeling, Data Analysis, Transformers, Machine Learning, Web\nScraping, Generative AI, Deep Learning, CNN,Computer Vision, Arti\ufb01cial General Intelligence, AI\nProjects\nDetecting Air pollution in Mongolia using NLP |Apify, T5 Bert , LSTM, Deberta LLM, Hugging Face DagsHub\n\u2022Collaborated with a 23- member team to develop and implement Environmental Sentinel on Social Media using\nsentiment analysisf or real-time air pollution monitoring in Mongolia.\n \u2022Applied 3 key techniques: Market Research ,Data Collection , and Preprocessing , leading to successful Model\nDevelopment\n\u2022Successfully scraped and collected 75,469 pieces of Mongolian text from diverse sources, including Mongolian social\nmedia platforms and news channels.\n", "original_text": "Experience working with\nTensorFlow, PyTorch, and scikit-learn and Expertise in Quantization, Fine -tuning and improving the models performance.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "82d84f09-7d30-435f-839f-bf83e19b907b", "node_type": "4", "metadata": {}, "hash": "b75d4ee1f34ec4f633bd750cdc013c8465514c53ed5e8a2c488c15f028158ec9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f8ee740d-cd33-4327-8880-b2e6157ff358", "node_type": "1", "metadata": {"window": "Josekutty Jose\nMachine Learning Engineer\n\u2642phone+91 9745949352 /envel\u2322pejosekuttyjose08@gmail.com /linkedinLinkedIn /githubGitHub DagsHub\u1f517LeetCode\nPro\ufb01le Summary\nHighly motivated Machine Learning Engineer over 6+ months of experience in Generative AI with a passion for building\nimpactful AI solutions.  Proven ability to design, develop, and deploy machine learning models across various domains.\n Skilled in data preprocessing, feature engineering, model selection, and hyperparameter tuning .  Experience working with\nTensorFlow, PyTorch, and scikit-learn and Expertise in Quantization, Fine -tuning and improving the models performance.\n Eager to contribute to a team environment and continuously learn new technologies.\n Technical Skills\nLanguages : Python, HTML, SQL, CSS, Neo4j, Cypher\nFrameworks : Tensor\ufb02ow, Langchain, Streamlit, Pytorch , Fastapi\nLibraries : Excel, Pandas, Numpy, Scipy, Scikit-learn\nTools & Platforms : Git, Dagshub,\nData Visualization : Power BI, Tableau , Matplotlib, Seaborn\nDomain Skills : NLP, Statistical Modeling, Predictive Modeling, Data Analysis, Transformers, Machine Learning, Web\nScraping, Generative AI, Deep Learning, CNN,Computer Vision, Arti\ufb01cial General Intelligence, AI\nProjects\nDetecting Air pollution in Mongolia using NLP |Apify, T5 Bert , LSTM, Deberta LLM, Hugging Face DagsHub\n\u2022Collaborated with a 23- member team to develop and implement Environmental Sentinel on Social Media using\nsentiment analysisf or real-time air pollution monitoring in Mongolia.\n", "original_text": "Skilled in data preprocessing, feature engineering, model selection, and hyperparameter tuning . "}, "hash": "0f2d255ed9d6cb91b7653130843a854a732b704c81d83cc4c09926f2bc63b953", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "73bc6480-4e8d-4faa-b739-34334eac9e0f", "node_type": "1", "metadata": {"window": "Proven ability to design, develop, and deploy machine learning models across various domains.\n Skilled in data preprocessing, feature engineering, model selection, and hyperparameter tuning .  Experience working with\nTensorFlow, PyTorch, and scikit-learn and Expertise in Quantization, Fine -tuning and improving the models performance.\n Eager to contribute to a team environment and continuously learn new technologies.\n Technical Skills\nLanguages : Python, HTML, SQL, CSS, Neo4j, Cypher\nFrameworks : Tensor\ufb02ow, Langchain, Streamlit, Pytorch , Fastapi\nLibraries : Excel, Pandas, Numpy, Scipy, Scikit-learn\nTools & Platforms : Git, Dagshub,\nData Visualization : Power BI, Tableau , Matplotlib, Seaborn\nDomain Skills : NLP, Statistical Modeling, Predictive Modeling, Data Analysis, Transformers, Machine Learning, Web\nScraping, Generative AI, Deep Learning, CNN,Computer Vision, Arti\ufb01cial General Intelligence, AI\nProjects\nDetecting Air pollution in Mongolia using NLP |Apify, T5 Bert , LSTM, Deberta LLM, Hugging Face DagsHub\n\u2022Collaborated with a 23- member team to develop and implement Environmental Sentinel on Social Media using\nsentiment analysisf or real-time air pollution monitoring in Mongolia.\n \u2022Applied 3 key techniques: Market Research ,Data Collection , and Preprocessing , leading to successful Model\nDevelopment\n\u2022Successfully scraped and collected 75,469 pieces of Mongolian text from diverse sources, including Mongolian social\nmedia platforms and news channels.\n \u2022Achieved sentiment analysis using LSTM with a focus on e\ufb03ciency, utilizing minimal epochs due to lengthy\nprocessing time. ", "original_text": "Eager to contribute to a team environment and continuously learn new technologies.\n"}, "hash": "3acd3ca3b320e8e62bae733624761ffe3470fdc4b67c0f3c26ceb919bfa45f6b", "class_name": "RelatedNodeInfo"}}, "text": "Experience working with\nTensorFlow, PyTorch, and scikit-learn and Expertise in Quantization, Fine -tuning and improving the models performance.\n", "start_char_idx": 494, "end_char_idx": 638, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "73bc6480-4e8d-4faa-b739-34334eac9e0f": {"__data__": {"id_": "73bc6480-4e8d-4faa-b739-34334eac9e0f", "embedding": null, "metadata": {"window": "Proven ability to design, develop, and deploy machine learning models across various domains.\n Skilled in data preprocessing, feature engineering, model selection, and hyperparameter tuning .  Experience working with\nTensorFlow, PyTorch, and scikit-learn and Expertise in Quantization, Fine -tuning and improving the models performance.\n Eager to contribute to a team environment and continuously learn new technologies.\n Technical Skills\nLanguages : Python, HTML, SQL, CSS, Neo4j, Cypher\nFrameworks : Tensor\ufb02ow, Langchain, Streamlit, Pytorch , Fastapi\nLibraries : Excel, Pandas, Numpy, Scipy, Scikit-learn\nTools & Platforms : Git, Dagshub,\nData Visualization : Power BI, Tableau , Matplotlib, Seaborn\nDomain Skills : NLP, Statistical Modeling, Predictive Modeling, Data Analysis, Transformers, Machine Learning, Web\nScraping, Generative AI, Deep Learning, CNN,Computer Vision, Arti\ufb01cial General Intelligence, AI\nProjects\nDetecting Air pollution in Mongolia using NLP |Apify, T5 Bert , LSTM, Deberta LLM, Hugging Face DagsHub\n\u2022Collaborated with a 23- member team to develop and implement Environmental Sentinel on Social Media using\nsentiment analysisf or real-time air pollution monitoring in Mongolia.\n \u2022Applied 3 key techniques: Market Research ,Data Collection , and Preprocessing , leading to successful Model\nDevelopment\n\u2022Successfully scraped and collected 75,469 pieces of Mongolian text from diverse sources, including Mongolian social\nmedia platforms and news channels.\n \u2022Achieved sentiment analysis using LSTM with a focus on e\ufb03ciency, utilizing minimal epochs due to lengthy\nprocessing time. ", "original_text": "Eager to contribute to a team environment and continuously learn new technologies.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "82d84f09-7d30-435f-839f-bf83e19b907b", "node_type": "4", "metadata": {}, "hash": "b75d4ee1f34ec4f633bd750cdc013c8465514c53ed5e8a2c488c15f028158ec9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "013b8180-fc65-41e2-a304-a79ad1e6dbbe", "node_type": "1", "metadata": {"window": "Josekutty Jose\nMachine Learning Engineer\n\u2642phone+91 9745949352 /envel\u2322pejosekuttyjose08@gmail.com /linkedinLinkedIn /githubGitHub DagsHub\u1f517LeetCode\nPro\ufb01le Summary\nHighly motivated Machine Learning Engineer over 6+ months of experience in Generative AI with a passion for building\nimpactful AI solutions.  Proven ability to design, develop, and deploy machine learning models across various domains.\n Skilled in data preprocessing, feature engineering, model selection, and hyperparameter tuning .  Experience working with\nTensorFlow, PyTorch, and scikit-learn and Expertise in Quantization, Fine -tuning and improving the models performance.\n Eager to contribute to a team environment and continuously learn new technologies.\n Technical Skills\nLanguages : Python, HTML, SQL, CSS, Neo4j, Cypher\nFrameworks : Tensor\ufb02ow, Langchain, Streamlit, Pytorch , Fastapi\nLibraries : Excel, Pandas, Numpy, Scipy, Scikit-learn\nTools & Platforms : Git, Dagshub,\nData Visualization : Power BI, Tableau , Matplotlib, Seaborn\nDomain Skills : NLP, Statistical Modeling, Predictive Modeling, Data Analysis, Transformers, Machine Learning, Web\nScraping, Generative AI, Deep Learning, CNN,Computer Vision, Arti\ufb01cial General Intelligence, AI\nProjects\nDetecting Air pollution in Mongolia using NLP |Apify, T5 Bert , LSTM, Deberta LLM, Hugging Face DagsHub\n\u2022Collaborated with a 23- member team to develop and implement Environmental Sentinel on Social Media using\nsentiment analysisf or real-time air pollution monitoring in Mongolia.\n \u2022Applied 3 key techniques: Market Research ,Data Collection , and Preprocessing , leading to successful Model\nDevelopment\n\u2022Successfully scraped and collected 75,469 pieces of Mongolian text from diverse sources, including Mongolian social\nmedia platforms and news channels.\n", "original_text": "Experience working with\nTensorFlow, PyTorch, and scikit-learn and Expertise in Quantization, Fine -tuning and improving the models performance.\n"}, "hash": "bf658989f0320032b00ac1c0606e66bf6f0d44f55c70a044f250c5c2f495867e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "877f6fa0-bc6d-4f2a-b190-1c3fddff9d25", "node_type": "1", "metadata": {"window": "Skilled in data preprocessing, feature engineering, model selection, and hyperparameter tuning .  Experience working with\nTensorFlow, PyTorch, and scikit-learn and Expertise in Quantization, Fine -tuning and improving the models performance.\n Eager to contribute to a team environment and continuously learn new technologies.\n Technical Skills\nLanguages : Python, HTML, SQL, CSS, Neo4j, Cypher\nFrameworks : Tensor\ufb02ow, Langchain, Streamlit, Pytorch , Fastapi\nLibraries : Excel, Pandas, Numpy, Scipy, Scikit-learn\nTools & Platforms : Git, Dagshub,\nData Visualization : Power BI, Tableau , Matplotlib, Seaborn\nDomain Skills : NLP, Statistical Modeling, Predictive Modeling, Data Analysis, Transformers, Machine Learning, Web\nScraping, Generative AI, Deep Learning, CNN,Computer Vision, Arti\ufb01cial General Intelligence, AI\nProjects\nDetecting Air pollution in Mongolia using NLP |Apify, T5 Bert , LSTM, Deberta LLM, Hugging Face DagsHub\n\u2022Collaborated with a 23- member team to develop and implement Environmental Sentinel on Social Media using\nsentiment analysisf or real-time air pollution monitoring in Mongolia.\n \u2022Applied 3 key techniques: Market Research ,Data Collection , and Preprocessing , leading to successful Model\nDevelopment\n\u2022Successfully scraped and collected 75,469 pieces of Mongolian text from diverse sources, including Mongolian social\nmedia platforms and news channels.\n \u2022Achieved sentiment analysis using LSTM with a focus on e\ufb03ciency, utilizing minimal epochs due to lengthy\nprocessing time.  Applied both categorical cross-entropy and binary cross-entropy loss functions, yielding 85%\naccuracy for categorical loss and 55% for binary loss.\n", "original_text": "Technical Skills\nLanguages : Python, HTML, SQL, CSS, Neo4j, Cypher\nFrameworks : Tensor\ufb02ow, Langchain, Streamlit, Pytorch , Fastapi\nLibraries : Excel, Pandas, Numpy, Scipy, Scikit-learn\nTools & Platforms : Git, Dagshub,\nData Visualization : Power BI, Tableau , Matplotlib, Seaborn\nDomain Skills : NLP, Statistical Modeling, Predictive Modeling, Data Analysis, Transformers, Machine Learning, Web\nScraping, Generative AI, Deep Learning, CNN,Computer Vision, Arti\ufb01cial General Intelligence, AI\nProjects\nDetecting Air pollution in Mongolia using NLP |Apify, T5 Bert , LSTM, Deberta LLM, Hugging Face DagsHub\n\u2022Collaborated with a 23- member team to develop and implement Environmental Sentinel on Social Media using\nsentiment analysisf or real-time air pollution monitoring in Mongolia.\n"}, "hash": "45a0a44dd8f6e1b72951c6fdabb0d4957536c55d1b05b14e5ca853161bf45de1", "class_name": "RelatedNodeInfo"}}, "text": "Eager to contribute to a team environment and continuously learn new technologies.\n", "start_char_idx": 638, "end_char_idx": 721, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "877f6fa0-bc6d-4f2a-b190-1c3fddff9d25": {"__data__": {"id_": "877f6fa0-bc6d-4f2a-b190-1c3fddff9d25", "embedding": null, "metadata": {"window": "Skilled in data preprocessing, feature engineering, model selection, and hyperparameter tuning .  Experience working with\nTensorFlow, PyTorch, and scikit-learn and Expertise in Quantization, Fine -tuning and improving the models performance.\n Eager to contribute to a team environment and continuously learn new technologies.\n Technical Skills\nLanguages : Python, HTML, SQL, CSS, Neo4j, Cypher\nFrameworks : Tensor\ufb02ow, Langchain, Streamlit, Pytorch , Fastapi\nLibraries : Excel, Pandas, Numpy, Scipy, Scikit-learn\nTools & Platforms : Git, Dagshub,\nData Visualization : Power BI, Tableau , Matplotlib, Seaborn\nDomain Skills : NLP, Statistical Modeling, Predictive Modeling, Data Analysis, Transformers, Machine Learning, Web\nScraping, Generative AI, Deep Learning, CNN,Computer Vision, Arti\ufb01cial General Intelligence, AI\nProjects\nDetecting Air pollution in Mongolia using NLP |Apify, T5 Bert , LSTM, Deberta LLM, Hugging Face DagsHub\n\u2022Collaborated with a 23- member team to develop and implement Environmental Sentinel on Social Media using\nsentiment analysisf or real-time air pollution monitoring in Mongolia.\n \u2022Applied 3 key techniques: Market Research ,Data Collection , and Preprocessing , leading to successful Model\nDevelopment\n\u2022Successfully scraped and collected 75,469 pieces of Mongolian text from diverse sources, including Mongolian social\nmedia platforms and news channels.\n \u2022Achieved sentiment analysis using LSTM with a focus on e\ufb03ciency, utilizing minimal epochs due to lengthy\nprocessing time.  Applied both categorical cross-entropy and binary cross-entropy loss functions, yielding 85%\naccuracy for categorical loss and 55% for binary loss.\n", "original_text": "Technical Skills\nLanguages : Python, HTML, SQL, CSS, Neo4j, Cypher\nFrameworks : Tensor\ufb02ow, Langchain, Streamlit, Pytorch , Fastapi\nLibraries : Excel, Pandas, Numpy, Scipy, Scikit-learn\nTools & Platforms : Git, Dagshub,\nData Visualization : Power BI, Tableau , Matplotlib, Seaborn\nDomain Skills : NLP, Statistical Modeling, Predictive Modeling, Data Analysis, Transformers, Machine Learning, Web\nScraping, Generative AI, Deep Learning, CNN,Computer Vision, Arti\ufb01cial General Intelligence, AI\nProjects\nDetecting Air pollution in Mongolia using NLP |Apify, T5 Bert , LSTM, Deberta LLM, Hugging Face DagsHub\n\u2022Collaborated with a 23- member team to develop and implement Environmental Sentinel on Social Media using\nsentiment analysisf or real-time air pollution monitoring in Mongolia.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "82d84f09-7d30-435f-839f-bf83e19b907b", "node_type": "4", "metadata": {}, "hash": "b75d4ee1f34ec4f633bd750cdc013c8465514c53ed5e8a2c488c15f028158ec9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "73bc6480-4e8d-4faa-b739-34334eac9e0f", "node_type": "1", "metadata": {"window": "Proven ability to design, develop, and deploy machine learning models across various domains.\n Skilled in data preprocessing, feature engineering, model selection, and hyperparameter tuning .  Experience working with\nTensorFlow, PyTorch, and scikit-learn and Expertise in Quantization, Fine -tuning and improving the models performance.\n Eager to contribute to a team environment and continuously learn new technologies.\n Technical Skills\nLanguages : Python, HTML, SQL, CSS, Neo4j, Cypher\nFrameworks : Tensor\ufb02ow, Langchain, Streamlit, Pytorch , Fastapi\nLibraries : Excel, Pandas, Numpy, Scipy, Scikit-learn\nTools & Platforms : Git, Dagshub,\nData Visualization : Power BI, Tableau , Matplotlib, Seaborn\nDomain Skills : NLP, Statistical Modeling, Predictive Modeling, Data Analysis, Transformers, Machine Learning, Web\nScraping, Generative AI, Deep Learning, CNN,Computer Vision, Arti\ufb01cial General Intelligence, AI\nProjects\nDetecting Air pollution in Mongolia using NLP |Apify, T5 Bert , LSTM, Deberta LLM, Hugging Face DagsHub\n\u2022Collaborated with a 23- member team to develop and implement Environmental Sentinel on Social Media using\nsentiment analysisf or real-time air pollution monitoring in Mongolia.\n \u2022Applied 3 key techniques: Market Research ,Data Collection , and Preprocessing , leading to successful Model\nDevelopment\n\u2022Successfully scraped and collected 75,469 pieces of Mongolian text from diverse sources, including Mongolian social\nmedia platforms and news channels.\n \u2022Achieved sentiment analysis using LSTM with a focus on e\ufb03ciency, utilizing minimal epochs due to lengthy\nprocessing time. ", "original_text": "Eager to contribute to a team environment and continuously learn new technologies.\n"}, "hash": "3acd3ca3b320e8e62bae733624761ffe3470fdc4b67c0f3c26ceb919bfa45f6b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d61d989a-cbfa-435a-a750-cfffed889895", "node_type": "1", "metadata": {"window": "Experience working with\nTensorFlow, PyTorch, and scikit-learn and Expertise in Quantization, Fine -tuning and improving the models performance.\n Eager to contribute to a team environment and continuously learn new technologies.\n Technical Skills\nLanguages : Python, HTML, SQL, CSS, Neo4j, Cypher\nFrameworks : Tensor\ufb02ow, Langchain, Streamlit, Pytorch , Fastapi\nLibraries : Excel, Pandas, Numpy, Scipy, Scikit-learn\nTools & Platforms : Git, Dagshub,\nData Visualization : Power BI, Tableau , Matplotlib, Seaborn\nDomain Skills : NLP, Statistical Modeling, Predictive Modeling, Data Analysis, Transformers, Machine Learning, Web\nScraping, Generative AI, Deep Learning, CNN,Computer Vision, Arti\ufb01cial General Intelligence, AI\nProjects\nDetecting Air pollution in Mongolia using NLP |Apify, T5 Bert , LSTM, Deberta LLM, Hugging Face DagsHub\n\u2022Collaborated with a 23- member team to develop and implement Environmental Sentinel on Social Media using\nsentiment analysisf or real-time air pollution monitoring in Mongolia.\n \u2022Applied 3 key techniques: Market Research ,Data Collection , and Preprocessing , leading to successful Model\nDevelopment\n\u2022Successfully scraped and collected 75,469 pieces of Mongolian text from diverse sources, including Mongolian social\nmedia platforms and news channels.\n \u2022Achieved sentiment analysis using LSTM with a focus on e\ufb03ciency, utilizing minimal epochs due to lengthy\nprocessing time.  Applied both categorical cross-entropy and binary cross-entropy loss functions, yielding 85%\naccuracy for categorical loss and 55% for binary loss.\n \u2022Leveraged advanced LoRA techniques to \ufb01ne-tune LLM model, resulting in a remarkable 4%increase in accuracy\nwithin a mere 3training epochs.\n", "original_text": "\u2022Applied 3 key techniques: Market Research ,Data Collection , and Preprocessing , leading to successful Model\nDevelopment\n\u2022Successfully scraped and collected 75,469 pieces of Mongolian text from diverse sources, including Mongolian social\nmedia platforms and news channels.\n"}, "hash": "7ccaff5857faa64d0a9823f702b7998f6089e0ada634643df49a943c9b79040f", "class_name": "RelatedNodeInfo"}}, "text": "Technical Skills\nLanguages : Python, HTML, SQL, CSS, Neo4j, Cypher\nFrameworks : Tensor\ufb02ow, Langchain, Streamlit, Pytorch , Fastapi\nLibraries : Excel, Pandas, Numpy, Scipy, Scikit-learn\nTools & Platforms : Git, Dagshub,\nData Visualization : Power BI, Tableau , Matplotlib, Seaborn\nDomain Skills : NLP, Statistical Modeling, Predictive Modeling, Data Analysis, Transformers, Machine Learning, Web\nScraping, Generative AI, Deep Learning, CNN,Computer Vision, Arti\ufb01cial General Intelligence, AI\nProjects\nDetecting Air pollution in Mongolia using NLP |Apify, T5 Bert , LSTM, Deberta LLM, Hugging Face DagsHub\n\u2022Collaborated with a 23- member team to develop and implement Environmental Sentinel on Social Media using\nsentiment analysisf or real-time air pollution monitoring in Mongolia.\n", "start_char_idx": 721, "end_char_idx": 1503, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d61d989a-cbfa-435a-a750-cfffed889895": {"__data__": {"id_": "d61d989a-cbfa-435a-a750-cfffed889895", "embedding": null, "metadata": {"window": "Experience working with\nTensorFlow, PyTorch, and scikit-learn and Expertise in Quantization, Fine -tuning and improving the models performance.\n Eager to contribute to a team environment and continuously learn new technologies.\n Technical Skills\nLanguages : Python, HTML, SQL, CSS, Neo4j, Cypher\nFrameworks : Tensor\ufb02ow, Langchain, Streamlit, Pytorch , Fastapi\nLibraries : Excel, Pandas, Numpy, Scipy, Scikit-learn\nTools & Platforms : Git, Dagshub,\nData Visualization : Power BI, Tableau , Matplotlib, Seaborn\nDomain Skills : NLP, Statistical Modeling, Predictive Modeling, Data Analysis, Transformers, Machine Learning, Web\nScraping, Generative AI, Deep Learning, CNN,Computer Vision, Arti\ufb01cial General Intelligence, AI\nProjects\nDetecting Air pollution in Mongolia using NLP |Apify, T5 Bert , LSTM, Deberta LLM, Hugging Face DagsHub\n\u2022Collaborated with a 23- member team to develop and implement Environmental Sentinel on Social Media using\nsentiment analysisf or real-time air pollution monitoring in Mongolia.\n \u2022Applied 3 key techniques: Market Research ,Data Collection , and Preprocessing , leading to successful Model\nDevelopment\n\u2022Successfully scraped and collected 75,469 pieces of Mongolian text from diverse sources, including Mongolian social\nmedia platforms and news channels.\n \u2022Achieved sentiment analysis using LSTM with a focus on e\ufb03ciency, utilizing minimal epochs due to lengthy\nprocessing time.  Applied both categorical cross-entropy and binary cross-entropy loss functions, yielding 85%\naccuracy for categorical loss and 55% for binary loss.\n \u2022Leveraged advanced LoRA techniques to \ufb01ne-tune LLM model, resulting in a remarkable 4%increase in accuracy\nwithin a mere 3training epochs.\n", "original_text": "\u2022Applied 3 key techniques: Market Research ,Data Collection , and Preprocessing , leading to successful Model\nDevelopment\n\u2022Successfully scraped and collected 75,469 pieces of Mongolian text from diverse sources, including Mongolian social\nmedia platforms and news channels.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "82d84f09-7d30-435f-839f-bf83e19b907b", "node_type": "4", "metadata": {}, "hash": "b75d4ee1f34ec4f633bd750cdc013c8465514c53ed5e8a2c488c15f028158ec9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "877f6fa0-bc6d-4f2a-b190-1c3fddff9d25", "node_type": "1", "metadata": {"window": "Skilled in data preprocessing, feature engineering, model selection, and hyperparameter tuning .  Experience working with\nTensorFlow, PyTorch, and scikit-learn and Expertise in Quantization, Fine -tuning and improving the models performance.\n Eager to contribute to a team environment and continuously learn new technologies.\n Technical Skills\nLanguages : Python, HTML, SQL, CSS, Neo4j, Cypher\nFrameworks : Tensor\ufb02ow, Langchain, Streamlit, Pytorch , Fastapi\nLibraries : Excel, Pandas, Numpy, Scipy, Scikit-learn\nTools & Platforms : Git, Dagshub,\nData Visualization : Power BI, Tableau , Matplotlib, Seaborn\nDomain Skills : NLP, Statistical Modeling, Predictive Modeling, Data Analysis, Transformers, Machine Learning, Web\nScraping, Generative AI, Deep Learning, CNN,Computer Vision, Arti\ufb01cial General Intelligence, AI\nProjects\nDetecting Air pollution in Mongolia using NLP |Apify, T5 Bert , LSTM, Deberta LLM, Hugging Face DagsHub\n\u2022Collaborated with a 23- member team to develop and implement Environmental Sentinel on Social Media using\nsentiment analysisf or real-time air pollution monitoring in Mongolia.\n \u2022Applied 3 key techniques: Market Research ,Data Collection , and Preprocessing , leading to successful Model\nDevelopment\n\u2022Successfully scraped and collected 75,469 pieces of Mongolian text from diverse sources, including Mongolian social\nmedia platforms and news channels.\n \u2022Achieved sentiment analysis using LSTM with a focus on e\ufb03ciency, utilizing minimal epochs due to lengthy\nprocessing time.  Applied both categorical cross-entropy and binary cross-entropy loss functions, yielding 85%\naccuracy for categorical loss and 55% for binary loss.\n", "original_text": "Technical Skills\nLanguages : Python, HTML, SQL, CSS, Neo4j, Cypher\nFrameworks : Tensor\ufb02ow, Langchain, Streamlit, Pytorch , Fastapi\nLibraries : Excel, Pandas, Numpy, Scipy, Scikit-learn\nTools & Platforms : Git, Dagshub,\nData Visualization : Power BI, Tableau , Matplotlib, Seaborn\nDomain Skills : NLP, Statistical Modeling, Predictive Modeling, Data Analysis, Transformers, Machine Learning, Web\nScraping, Generative AI, Deep Learning, CNN,Computer Vision, Arti\ufb01cial General Intelligence, AI\nProjects\nDetecting Air pollution in Mongolia using NLP |Apify, T5 Bert , LSTM, Deberta LLM, Hugging Face DagsHub\n\u2022Collaborated with a 23- member team to develop and implement Environmental Sentinel on Social Media using\nsentiment analysisf or real-time air pollution monitoring in Mongolia.\n"}, "hash": "45a0a44dd8f6e1b72951c6fdabb0d4957536c55d1b05b14e5ca853161bf45de1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3fa3a56e-1188-41c5-8c58-c43d303f8539", "node_type": "1", "metadata": {"window": "Eager to contribute to a team environment and continuously learn new technologies.\n Technical Skills\nLanguages : Python, HTML, SQL, CSS, Neo4j, Cypher\nFrameworks : Tensor\ufb02ow, Langchain, Streamlit, Pytorch , Fastapi\nLibraries : Excel, Pandas, Numpy, Scipy, Scikit-learn\nTools & Platforms : Git, Dagshub,\nData Visualization : Power BI, Tableau , Matplotlib, Seaborn\nDomain Skills : NLP, Statistical Modeling, Predictive Modeling, Data Analysis, Transformers, Machine Learning, Web\nScraping, Generative AI, Deep Learning, CNN,Computer Vision, Arti\ufb01cial General Intelligence, AI\nProjects\nDetecting Air pollution in Mongolia using NLP |Apify, T5 Bert , LSTM, Deberta LLM, Hugging Face DagsHub\n\u2022Collaborated with a 23- member team to develop and implement Environmental Sentinel on Social Media using\nsentiment analysisf or real-time air pollution monitoring in Mongolia.\n \u2022Applied 3 key techniques: Market Research ,Data Collection , and Preprocessing , leading to successful Model\nDevelopment\n\u2022Successfully scraped and collected 75,469 pieces of Mongolian text from diverse sources, including Mongolian social\nmedia platforms and news channels.\n \u2022Achieved sentiment analysis using LSTM with a focus on e\ufb03ciency, utilizing minimal epochs due to lengthy\nprocessing time.  Applied both categorical cross-entropy and binary cross-entropy loss functions, yielding 85%\naccuracy for categorical loss and 55% for binary loss.\n \u2022Leveraged advanced LoRA techniques to \ufb01ne-tune LLM model, resulting in a remarkable 4%increase in accuracy\nwithin a mere 3training epochs.\n Interview Preparation Chatbot: |gtts , speech recognition , Chroma, DuckDB, Gemini. ", "original_text": "\u2022Achieved sentiment analysis using LSTM with a focus on e\ufb03ciency, utilizing minimal epochs due to lengthy\nprocessing time. "}, "hash": "27823941240b82091d25ca3d2cf3d227c16e20160e50d0f33f4beef4f0f70f9f", "class_name": "RelatedNodeInfo"}}, "text": "\u2022Applied 3 key techniques: Market Research ,Data Collection , and Preprocessing , leading to successful Model\nDevelopment\n\u2022Successfully scraped and collected 75,469 pieces of Mongolian text from diverse sources, including Mongolian social\nmedia platforms and news channels.\n", "start_char_idx": 1503, "end_char_idx": 1777, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3fa3a56e-1188-41c5-8c58-c43d303f8539": {"__data__": {"id_": "3fa3a56e-1188-41c5-8c58-c43d303f8539", "embedding": null, "metadata": {"window": "Eager to contribute to a team environment and continuously learn new technologies.\n Technical Skills\nLanguages : Python, HTML, SQL, CSS, Neo4j, Cypher\nFrameworks : Tensor\ufb02ow, Langchain, Streamlit, Pytorch , Fastapi\nLibraries : Excel, Pandas, Numpy, Scipy, Scikit-learn\nTools & Platforms : Git, Dagshub,\nData Visualization : Power BI, Tableau , Matplotlib, Seaborn\nDomain Skills : NLP, Statistical Modeling, Predictive Modeling, Data Analysis, Transformers, Machine Learning, Web\nScraping, Generative AI, Deep Learning, CNN,Computer Vision, Arti\ufb01cial General Intelligence, AI\nProjects\nDetecting Air pollution in Mongolia using NLP |Apify, T5 Bert , LSTM, Deberta LLM, Hugging Face DagsHub\n\u2022Collaborated with a 23- member team to develop and implement Environmental Sentinel on Social Media using\nsentiment analysisf or real-time air pollution monitoring in Mongolia.\n \u2022Applied 3 key techniques: Market Research ,Data Collection , and Preprocessing , leading to successful Model\nDevelopment\n\u2022Successfully scraped and collected 75,469 pieces of Mongolian text from diverse sources, including Mongolian social\nmedia platforms and news channels.\n \u2022Achieved sentiment analysis using LSTM with a focus on e\ufb03ciency, utilizing minimal epochs due to lengthy\nprocessing time.  Applied both categorical cross-entropy and binary cross-entropy loss functions, yielding 85%\naccuracy for categorical loss and 55% for binary loss.\n \u2022Leveraged advanced LoRA techniques to \ufb01ne-tune LLM model, resulting in a remarkable 4%increase in accuracy\nwithin a mere 3training epochs.\n Interview Preparation Chatbot: |gtts , speech recognition , Chroma, DuckDB, Gemini. ", "original_text": "\u2022Achieved sentiment analysis using LSTM with a focus on e\ufb03ciency, utilizing minimal epochs due to lengthy\nprocessing time. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "82d84f09-7d30-435f-839f-bf83e19b907b", "node_type": "4", "metadata": {}, "hash": "b75d4ee1f34ec4f633bd750cdc013c8465514c53ed5e8a2c488c15f028158ec9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d61d989a-cbfa-435a-a750-cfffed889895", "node_type": "1", "metadata": {"window": "Experience working with\nTensorFlow, PyTorch, and scikit-learn and Expertise in Quantization, Fine -tuning and improving the models performance.\n Eager to contribute to a team environment and continuously learn new technologies.\n Technical Skills\nLanguages : Python, HTML, SQL, CSS, Neo4j, Cypher\nFrameworks : Tensor\ufb02ow, Langchain, Streamlit, Pytorch , Fastapi\nLibraries : Excel, Pandas, Numpy, Scipy, Scikit-learn\nTools & Platforms : Git, Dagshub,\nData Visualization : Power BI, Tableau , Matplotlib, Seaborn\nDomain Skills : NLP, Statistical Modeling, Predictive Modeling, Data Analysis, Transformers, Machine Learning, Web\nScraping, Generative AI, Deep Learning, CNN,Computer Vision, Arti\ufb01cial General Intelligence, AI\nProjects\nDetecting Air pollution in Mongolia using NLP |Apify, T5 Bert , LSTM, Deberta LLM, Hugging Face DagsHub\n\u2022Collaborated with a 23- member team to develop and implement Environmental Sentinel on Social Media using\nsentiment analysisf or real-time air pollution monitoring in Mongolia.\n \u2022Applied 3 key techniques: Market Research ,Data Collection , and Preprocessing , leading to successful Model\nDevelopment\n\u2022Successfully scraped and collected 75,469 pieces of Mongolian text from diverse sources, including Mongolian social\nmedia platforms and news channels.\n \u2022Achieved sentiment analysis using LSTM with a focus on e\ufb03ciency, utilizing minimal epochs due to lengthy\nprocessing time.  Applied both categorical cross-entropy and binary cross-entropy loss functions, yielding 85%\naccuracy for categorical loss and 55% for binary loss.\n \u2022Leveraged advanced LoRA techniques to \ufb01ne-tune LLM model, resulting in a remarkable 4%increase in accuracy\nwithin a mere 3training epochs.\n", "original_text": "\u2022Applied 3 key techniques: Market Research ,Data Collection , and Preprocessing , leading to successful Model\nDevelopment\n\u2022Successfully scraped and collected 75,469 pieces of Mongolian text from diverse sources, including Mongolian social\nmedia platforms and news channels.\n"}, "hash": "7ccaff5857faa64d0a9823f702b7998f6089e0ada634643df49a943c9b79040f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7216234e-c894-44bc-99d3-5f8db881c192", "node_type": "1", "metadata": {"window": "Technical Skills\nLanguages : Python, HTML, SQL, CSS, Neo4j, Cypher\nFrameworks : Tensor\ufb02ow, Langchain, Streamlit, Pytorch , Fastapi\nLibraries : Excel, Pandas, Numpy, Scipy, Scikit-learn\nTools & Platforms : Git, Dagshub,\nData Visualization : Power BI, Tableau , Matplotlib, Seaborn\nDomain Skills : NLP, Statistical Modeling, Predictive Modeling, Data Analysis, Transformers, Machine Learning, Web\nScraping, Generative AI, Deep Learning, CNN,Computer Vision, Arti\ufb01cial General Intelligence, AI\nProjects\nDetecting Air pollution in Mongolia using NLP |Apify, T5 Bert , LSTM, Deberta LLM, Hugging Face DagsHub\n\u2022Collaborated with a 23- member team to develop and implement Environmental Sentinel on Social Media using\nsentiment analysisf or real-time air pollution monitoring in Mongolia.\n \u2022Applied 3 key techniques: Market Research ,Data Collection , and Preprocessing , leading to successful Model\nDevelopment\n\u2022Successfully scraped and collected 75,469 pieces of Mongolian text from diverse sources, including Mongolian social\nmedia platforms and news channels.\n \u2022Achieved sentiment analysis using LSTM with a focus on e\ufb03ciency, utilizing minimal epochs due to lengthy\nprocessing time.  Applied both categorical cross-entropy and binary cross-entropy loss functions, yielding 85%\naccuracy for categorical loss and 55% for binary loss.\n \u2022Leveraged advanced LoRA techniques to \ufb01ne-tune LLM model, resulting in a remarkable 4%increase in accuracy\nwithin a mere 3training epochs.\n Interview Preparation Chatbot: |gtts , speech recognition , Chroma, DuckDB, Gemini.  Live |DagsHub\n\u2022Collaborated with a team of 30to develop a Chatbot web application for Interview preparation.\n", "original_text": "Applied both categorical cross-entropy and binary cross-entropy loss functions, yielding 85%\naccuracy for categorical loss and 55% for binary loss.\n"}, "hash": "19106c6b5985f02e285a3cd09d593c07408057234b17875097fcb5eef2eeeb1e", "class_name": "RelatedNodeInfo"}}, "text": "\u2022Achieved sentiment analysis using LSTM with a focus on e\ufb03ciency, utilizing minimal epochs due to lengthy\nprocessing time. ", "start_char_idx": 1777, "end_char_idx": 1900, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7216234e-c894-44bc-99d3-5f8db881c192": {"__data__": {"id_": "7216234e-c894-44bc-99d3-5f8db881c192", "embedding": null, "metadata": {"window": "Technical Skills\nLanguages : Python, HTML, SQL, CSS, Neo4j, Cypher\nFrameworks : Tensor\ufb02ow, Langchain, Streamlit, Pytorch , Fastapi\nLibraries : Excel, Pandas, Numpy, Scipy, Scikit-learn\nTools & Platforms : Git, Dagshub,\nData Visualization : Power BI, Tableau , Matplotlib, Seaborn\nDomain Skills : NLP, Statistical Modeling, Predictive Modeling, Data Analysis, Transformers, Machine Learning, Web\nScraping, Generative AI, Deep Learning, CNN,Computer Vision, Arti\ufb01cial General Intelligence, AI\nProjects\nDetecting Air pollution in Mongolia using NLP |Apify, T5 Bert , LSTM, Deberta LLM, Hugging Face DagsHub\n\u2022Collaborated with a 23- member team to develop and implement Environmental Sentinel on Social Media using\nsentiment analysisf or real-time air pollution monitoring in Mongolia.\n \u2022Applied 3 key techniques: Market Research ,Data Collection , and Preprocessing , leading to successful Model\nDevelopment\n\u2022Successfully scraped and collected 75,469 pieces of Mongolian text from diverse sources, including Mongolian social\nmedia platforms and news channels.\n \u2022Achieved sentiment analysis using LSTM with a focus on e\ufb03ciency, utilizing minimal epochs due to lengthy\nprocessing time.  Applied both categorical cross-entropy and binary cross-entropy loss functions, yielding 85%\naccuracy for categorical loss and 55% for binary loss.\n \u2022Leveraged advanced LoRA techniques to \ufb01ne-tune LLM model, resulting in a remarkable 4%increase in accuracy\nwithin a mere 3training epochs.\n Interview Preparation Chatbot: |gtts , speech recognition , Chroma, DuckDB, Gemini.  Live |DagsHub\n\u2022Collaborated with a team of 30to develop a Chatbot web application for Interview preparation.\n", "original_text": "Applied both categorical cross-entropy and binary cross-entropy loss functions, yielding 85%\naccuracy for categorical loss and 55% for binary loss.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "82d84f09-7d30-435f-839f-bf83e19b907b", "node_type": "4", "metadata": {}, "hash": "b75d4ee1f34ec4f633bd750cdc013c8465514c53ed5e8a2c488c15f028158ec9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3fa3a56e-1188-41c5-8c58-c43d303f8539", "node_type": "1", "metadata": {"window": "Eager to contribute to a team environment and continuously learn new technologies.\n Technical Skills\nLanguages : Python, HTML, SQL, CSS, Neo4j, Cypher\nFrameworks : Tensor\ufb02ow, Langchain, Streamlit, Pytorch , Fastapi\nLibraries : Excel, Pandas, Numpy, Scipy, Scikit-learn\nTools & Platforms : Git, Dagshub,\nData Visualization : Power BI, Tableau , Matplotlib, Seaborn\nDomain Skills : NLP, Statistical Modeling, Predictive Modeling, Data Analysis, Transformers, Machine Learning, Web\nScraping, Generative AI, Deep Learning, CNN,Computer Vision, Arti\ufb01cial General Intelligence, AI\nProjects\nDetecting Air pollution in Mongolia using NLP |Apify, T5 Bert , LSTM, Deberta LLM, Hugging Face DagsHub\n\u2022Collaborated with a 23- member team to develop and implement Environmental Sentinel on Social Media using\nsentiment analysisf or real-time air pollution monitoring in Mongolia.\n \u2022Applied 3 key techniques: Market Research ,Data Collection , and Preprocessing , leading to successful Model\nDevelopment\n\u2022Successfully scraped and collected 75,469 pieces of Mongolian text from diverse sources, including Mongolian social\nmedia platforms and news channels.\n \u2022Achieved sentiment analysis using LSTM with a focus on e\ufb03ciency, utilizing minimal epochs due to lengthy\nprocessing time.  Applied both categorical cross-entropy and binary cross-entropy loss functions, yielding 85%\naccuracy for categorical loss and 55% for binary loss.\n \u2022Leveraged advanced LoRA techniques to \ufb01ne-tune LLM model, resulting in a remarkable 4%increase in accuracy\nwithin a mere 3training epochs.\n Interview Preparation Chatbot: |gtts , speech recognition , Chroma, DuckDB, Gemini. ", "original_text": "\u2022Achieved sentiment analysis using LSTM with a focus on e\ufb03ciency, utilizing minimal epochs due to lengthy\nprocessing time. "}, "hash": "27823941240b82091d25ca3d2cf3d227c16e20160e50d0f33f4beef4f0f70f9f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "807d0e15-d9b4-4d49-b92a-53b51b9f7ba6", "node_type": "1", "metadata": {"window": "\u2022Applied 3 key techniques: Market Research ,Data Collection , and Preprocessing , leading to successful Model\nDevelopment\n\u2022Successfully scraped and collected 75,469 pieces of Mongolian text from diverse sources, including Mongolian social\nmedia platforms and news channels.\n \u2022Achieved sentiment analysis using LSTM with a focus on e\ufb03ciency, utilizing minimal epochs due to lengthy\nprocessing time.  Applied both categorical cross-entropy and binary cross-entropy loss functions, yielding 85%\naccuracy for categorical loss and 55% for binary loss.\n \u2022Leveraged advanced LoRA techniques to \ufb01ne-tune LLM model, resulting in a remarkable 4%increase in accuracy\nwithin a mere 3training epochs.\n Interview Preparation Chatbot: |gtts , speech recognition , Chroma, DuckDB, Gemini.  Live |DagsHub\n\u2022Collaborated with a team of 30to develop a Chatbot web application for Interview preparation.\n \u2022Collected 7000 questions and answers from di\ufb00erent sources for Non technical Jobs using Beautiful Soup(Web\nScraping).\n", "original_text": "\u2022Leveraged advanced LoRA techniques to \ufb01ne-tune LLM model, resulting in a remarkable 4%increase in accuracy\nwithin a mere 3training epochs.\n"}, "hash": "5ccb500f7f13f6146fb4647d429be0f0eb26fb6f9cf2014d8d9717f705c55793", "class_name": "RelatedNodeInfo"}}, "text": "Applied both categorical cross-entropy and binary cross-entropy loss functions, yielding 85%\naccuracy for categorical loss and 55% for binary loss.\n", "start_char_idx": 1900, "end_char_idx": 2048, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "807d0e15-d9b4-4d49-b92a-53b51b9f7ba6": {"__data__": {"id_": "807d0e15-d9b4-4d49-b92a-53b51b9f7ba6", "embedding": null, "metadata": {"window": "\u2022Applied 3 key techniques: Market Research ,Data Collection , and Preprocessing , leading to successful Model\nDevelopment\n\u2022Successfully scraped and collected 75,469 pieces of Mongolian text from diverse sources, including Mongolian social\nmedia platforms and news channels.\n \u2022Achieved sentiment analysis using LSTM with a focus on e\ufb03ciency, utilizing minimal epochs due to lengthy\nprocessing time.  Applied both categorical cross-entropy and binary cross-entropy loss functions, yielding 85%\naccuracy for categorical loss and 55% for binary loss.\n \u2022Leveraged advanced LoRA techniques to \ufb01ne-tune LLM model, resulting in a remarkable 4%increase in accuracy\nwithin a mere 3training epochs.\n Interview Preparation Chatbot: |gtts , speech recognition , Chroma, DuckDB, Gemini.  Live |DagsHub\n\u2022Collaborated with a team of 30to develop a Chatbot web application for Interview preparation.\n \u2022Collected 7000 questions and answers from di\ufb00erent sources for Non technical Jobs using Beautiful Soup(Web\nScraping).\n", "original_text": "\u2022Leveraged advanced LoRA techniques to \ufb01ne-tune LLM model, resulting in a remarkable 4%increase in accuracy\nwithin a mere 3training epochs.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "82d84f09-7d30-435f-839f-bf83e19b907b", "node_type": "4", "metadata": {}, "hash": "b75d4ee1f34ec4f633bd750cdc013c8465514c53ed5e8a2c488c15f028158ec9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7216234e-c894-44bc-99d3-5f8db881c192", "node_type": "1", "metadata": {"window": "Technical Skills\nLanguages : Python, HTML, SQL, CSS, Neo4j, Cypher\nFrameworks : Tensor\ufb02ow, Langchain, Streamlit, Pytorch , Fastapi\nLibraries : Excel, Pandas, Numpy, Scipy, Scikit-learn\nTools & Platforms : Git, Dagshub,\nData Visualization : Power BI, Tableau , Matplotlib, Seaborn\nDomain Skills : NLP, Statistical Modeling, Predictive Modeling, Data Analysis, Transformers, Machine Learning, Web\nScraping, Generative AI, Deep Learning, CNN,Computer Vision, Arti\ufb01cial General Intelligence, AI\nProjects\nDetecting Air pollution in Mongolia using NLP |Apify, T5 Bert , LSTM, Deberta LLM, Hugging Face DagsHub\n\u2022Collaborated with a 23- member team to develop and implement Environmental Sentinel on Social Media using\nsentiment analysisf or real-time air pollution monitoring in Mongolia.\n \u2022Applied 3 key techniques: Market Research ,Data Collection , and Preprocessing , leading to successful Model\nDevelopment\n\u2022Successfully scraped and collected 75,469 pieces of Mongolian text from diverse sources, including Mongolian social\nmedia platforms and news channels.\n \u2022Achieved sentiment analysis using LSTM with a focus on e\ufb03ciency, utilizing minimal epochs due to lengthy\nprocessing time.  Applied both categorical cross-entropy and binary cross-entropy loss functions, yielding 85%\naccuracy for categorical loss and 55% for binary loss.\n \u2022Leveraged advanced LoRA techniques to \ufb01ne-tune LLM model, resulting in a remarkable 4%increase in accuracy\nwithin a mere 3training epochs.\n Interview Preparation Chatbot: |gtts , speech recognition , Chroma, DuckDB, Gemini.  Live |DagsHub\n\u2022Collaborated with a team of 30to develop a Chatbot web application for Interview preparation.\n", "original_text": "Applied both categorical cross-entropy and binary cross-entropy loss functions, yielding 85%\naccuracy for categorical loss and 55% for binary loss.\n"}, "hash": "19106c6b5985f02e285a3cd09d593c07408057234b17875097fcb5eef2eeeb1e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "9001860d-70bd-435a-9d19-9be6a541e917", "node_type": "1", "metadata": {"window": "\u2022Achieved sentiment analysis using LSTM with a focus on e\ufb03ciency, utilizing minimal epochs due to lengthy\nprocessing time.  Applied both categorical cross-entropy and binary cross-entropy loss functions, yielding 85%\naccuracy for categorical loss and 55% for binary loss.\n \u2022Leveraged advanced LoRA techniques to \ufb01ne-tune LLM model, resulting in a remarkable 4%increase in accuracy\nwithin a mere 3training epochs.\n Interview Preparation Chatbot: |gtts , speech recognition , Chroma, DuckDB, Gemini.  Live |DagsHub\n\u2022Collaborated with a team of 30to develop a Chatbot web application for Interview preparation.\n \u2022Collected 7000 questions and answers from di\ufb00erent sources for Non technical Jobs using Beautiful Soup(Web\nScraping).\n \u2022Implemented Speech Recognition technology to enable seamless interaction, enhancing the user experience\nthrough voice driven commands and responses using gttsand speech recognition.\n", "original_text": "Interview Preparation Chatbot: |gtts , speech recognition , Chroma, DuckDB, Gemini. "}, "hash": "20648bf5af2d99d9a510eb7bff23c169637fdffeba58e071808948d477858357", "class_name": "RelatedNodeInfo"}}, "text": "\u2022Leveraged advanced LoRA techniques to \ufb01ne-tune LLM model, resulting in a remarkable 4%increase in accuracy\nwithin a mere 3training epochs.\n", "start_char_idx": 2048, "end_char_idx": 2188, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9001860d-70bd-435a-9d19-9be6a541e917": {"__data__": {"id_": "9001860d-70bd-435a-9d19-9be6a541e917", "embedding": null, "metadata": {"window": "\u2022Achieved sentiment analysis using LSTM with a focus on e\ufb03ciency, utilizing minimal epochs due to lengthy\nprocessing time.  Applied both categorical cross-entropy and binary cross-entropy loss functions, yielding 85%\naccuracy for categorical loss and 55% for binary loss.\n \u2022Leveraged advanced LoRA techniques to \ufb01ne-tune LLM model, resulting in a remarkable 4%increase in accuracy\nwithin a mere 3training epochs.\n Interview Preparation Chatbot: |gtts , speech recognition , Chroma, DuckDB, Gemini.  Live |DagsHub\n\u2022Collaborated with a team of 30to develop a Chatbot web application for Interview preparation.\n \u2022Collected 7000 questions and answers from di\ufb00erent sources for Non technical Jobs using Beautiful Soup(Web\nScraping).\n \u2022Implemented Speech Recognition technology to enable seamless interaction, enhancing the user experience\nthrough voice driven commands and responses using gttsand speech recognition.\n", "original_text": "Interview Preparation Chatbot: |gtts , speech recognition , Chroma, DuckDB, Gemini. "}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "82d84f09-7d30-435f-839f-bf83e19b907b", "node_type": "4", "metadata": {}, "hash": "b75d4ee1f34ec4f633bd750cdc013c8465514c53ed5e8a2c488c15f028158ec9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "807d0e15-d9b4-4d49-b92a-53b51b9f7ba6", "node_type": "1", "metadata": {"window": "\u2022Applied 3 key techniques: Market Research ,Data Collection , and Preprocessing , leading to successful Model\nDevelopment\n\u2022Successfully scraped and collected 75,469 pieces of Mongolian text from diverse sources, including Mongolian social\nmedia platforms and news channels.\n \u2022Achieved sentiment analysis using LSTM with a focus on e\ufb03ciency, utilizing minimal epochs due to lengthy\nprocessing time.  Applied both categorical cross-entropy and binary cross-entropy loss functions, yielding 85%\naccuracy for categorical loss and 55% for binary loss.\n \u2022Leveraged advanced LoRA techniques to \ufb01ne-tune LLM model, resulting in a remarkable 4%increase in accuracy\nwithin a mere 3training epochs.\n Interview Preparation Chatbot: |gtts , speech recognition , Chroma, DuckDB, Gemini.  Live |DagsHub\n\u2022Collaborated with a team of 30to develop a Chatbot web application for Interview preparation.\n \u2022Collected 7000 questions and answers from di\ufb00erent sources for Non technical Jobs using Beautiful Soup(Web\nScraping).\n", "original_text": "\u2022Leveraged advanced LoRA techniques to \ufb01ne-tune LLM model, resulting in a remarkable 4%increase in accuracy\nwithin a mere 3training epochs.\n"}, "hash": "5ccb500f7f13f6146fb4647d429be0f0eb26fb6f9cf2014d8d9717f705c55793", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "0afe309c-44d9-40db-a58e-530a002c8b55", "node_type": "1", "metadata": {"window": "Applied both categorical cross-entropy and binary cross-entropy loss functions, yielding 85%\naccuracy for categorical loss and 55% for binary loss.\n \u2022Leveraged advanced LoRA techniques to \ufb01ne-tune LLM model, resulting in a remarkable 4%increase in accuracy\nwithin a mere 3training epochs.\n Interview Preparation Chatbot: |gtts , speech recognition , Chroma, DuckDB, Gemini.  Live |DagsHub\n\u2022Collaborated with a team of 30to develop a Chatbot web application for Interview preparation.\n \u2022Collected 7000 questions and answers from di\ufb00erent sources for Non technical Jobs using Beautiful Soup(Web\nScraping).\n \u2022Implemented Speech Recognition technology to enable seamless interaction, enhancing the user experience\nthrough voice driven commands and responses using gttsand speech recognition.\n \u2022Implemented RAG (Retrieval-Augmented Generation) with Chroma and DuckDB as vector databases for e\ufb03cient\nretrieval\n\u2022The answers were evaluated using the vector similarity check.\n", "original_text": "Live |DagsHub\n\u2022Collaborated with a team of 30to develop a Chatbot web application for Interview preparation.\n"}, "hash": "b9b48789b175b7ab6ebd47f933e41f77b40b7d9d5f4981778a56909df20a8754", "class_name": "RelatedNodeInfo"}}, "text": "Interview Preparation Chatbot: |gtts , speech recognition , Chroma, DuckDB, Gemini. ", "start_char_idx": 2188, "end_char_idx": 2272, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0afe309c-44d9-40db-a58e-530a002c8b55": {"__data__": {"id_": "0afe309c-44d9-40db-a58e-530a002c8b55", "embedding": null, "metadata": {"window": "Applied both categorical cross-entropy and binary cross-entropy loss functions, yielding 85%\naccuracy for categorical loss and 55% for binary loss.\n \u2022Leveraged advanced LoRA techniques to \ufb01ne-tune LLM model, resulting in a remarkable 4%increase in accuracy\nwithin a mere 3training epochs.\n Interview Preparation Chatbot: |gtts , speech recognition , Chroma, DuckDB, Gemini.  Live |DagsHub\n\u2022Collaborated with a team of 30to develop a Chatbot web application for Interview preparation.\n \u2022Collected 7000 questions and answers from di\ufb00erent sources for Non technical Jobs using Beautiful Soup(Web\nScraping).\n \u2022Implemented Speech Recognition technology to enable seamless interaction, enhancing the user experience\nthrough voice driven commands and responses using gttsand speech recognition.\n \u2022Implemented RAG (Retrieval-Augmented Generation) with Chroma and DuckDB as vector databases for e\ufb03cient\nretrieval\n\u2022The answers were evaluated using the vector similarity check.\n", "original_text": "Live |DagsHub\n\u2022Collaborated with a team of 30to develop a Chatbot web application for Interview preparation.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "82d84f09-7d30-435f-839f-bf83e19b907b", "node_type": "4", "metadata": {}, "hash": "b75d4ee1f34ec4f633bd750cdc013c8465514c53ed5e8a2c488c15f028158ec9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "9001860d-70bd-435a-9d19-9be6a541e917", "node_type": "1", "metadata": {"window": "\u2022Achieved sentiment analysis using LSTM with a focus on e\ufb03ciency, utilizing minimal epochs due to lengthy\nprocessing time.  Applied both categorical cross-entropy and binary cross-entropy loss functions, yielding 85%\naccuracy for categorical loss and 55% for binary loss.\n \u2022Leveraged advanced LoRA techniques to \ufb01ne-tune LLM model, resulting in a remarkable 4%increase in accuracy\nwithin a mere 3training epochs.\n Interview Preparation Chatbot: |gtts , speech recognition , Chroma, DuckDB, Gemini.  Live |DagsHub\n\u2022Collaborated with a team of 30to develop a Chatbot web application for Interview preparation.\n \u2022Collected 7000 questions and answers from di\ufb00erent sources for Non technical Jobs using Beautiful Soup(Web\nScraping).\n \u2022Implemented Speech Recognition technology to enable seamless interaction, enhancing the user experience\nthrough voice driven commands and responses using gttsand speech recognition.\n", "original_text": "Interview Preparation Chatbot: |gtts , speech recognition , Chroma, DuckDB, Gemini. "}, "hash": "20648bf5af2d99d9a510eb7bff23c169637fdffeba58e071808948d477858357", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "79b1ee48-d786-4146-a49c-c444f5487301", "node_type": "1", "metadata": {"window": "\u2022Leveraged advanced LoRA techniques to \ufb01ne-tune LLM model, resulting in a remarkable 4%increase in accuracy\nwithin a mere 3training epochs.\n Interview Preparation Chatbot: |gtts , speech recognition , Chroma, DuckDB, Gemini.  Live |DagsHub\n\u2022Collaborated with a team of 30to develop a Chatbot web application for Interview preparation.\n \u2022Collected 7000 questions and answers from di\ufb00erent sources for Non technical Jobs using Beautiful Soup(Web\nScraping).\n \u2022Implemented Speech Recognition technology to enable seamless interaction, enhancing the user experience\nthrough voice driven commands and responses using gttsand speech recognition.\n \u2022Implemented RAG (Retrieval-Augmented Generation) with Chroma and DuckDB as vector databases for e\ufb03cient\nretrieval\n\u2022The answers were evaluated using the vector similarity check.\n GemInsights: AI Automated Analyst |Autoviz, Gemini, llama index, Trulens Live |GitHub\n\u2022Collaborated with a team of 6during a Gemini hackathon organized by LabLab, developing an automatic EDA\n(Exploratory Data Analysis) application using Gemini models.\n", "original_text": "\u2022Collected 7000 questions and answers from di\ufb00erent sources for Non technical Jobs using Beautiful Soup(Web\nScraping).\n"}, "hash": "5086d7e453d88fc57c2a24214f791e52582025903b56487f9b852195bad8fbb1", "class_name": "RelatedNodeInfo"}}, "text": "Live |DagsHub\n\u2022Collaborated with a team of 30to develop a Chatbot web application for Interview preparation.\n", "start_char_idx": 2272, "end_char_idx": 2381, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "79b1ee48-d786-4146-a49c-c444f5487301": {"__data__": {"id_": "79b1ee48-d786-4146-a49c-c444f5487301", "embedding": null, "metadata": {"window": "\u2022Leveraged advanced LoRA techniques to \ufb01ne-tune LLM model, resulting in a remarkable 4%increase in accuracy\nwithin a mere 3training epochs.\n Interview Preparation Chatbot: |gtts , speech recognition , Chroma, DuckDB, Gemini.  Live |DagsHub\n\u2022Collaborated with a team of 30to develop a Chatbot web application for Interview preparation.\n \u2022Collected 7000 questions and answers from di\ufb00erent sources for Non technical Jobs using Beautiful Soup(Web\nScraping).\n \u2022Implemented Speech Recognition technology to enable seamless interaction, enhancing the user experience\nthrough voice driven commands and responses using gttsand speech recognition.\n \u2022Implemented RAG (Retrieval-Augmented Generation) with Chroma and DuckDB as vector databases for e\ufb03cient\nretrieval\n\u2022The answers were evaluated using the vector similarity check.\n GemInsights: AI Automated Analyst |Autoviz, Gemini, llama index, Trulens Live |GitHub\n\u2022Collaborated with a team of 6during a Gemini hackathon organized by LabLab, developing an automatic EDA\n(Exploratory Data Analysis) application using Gemini models.\n", "original_text": "\u2022Collected 7000 questions and answers from di\ufb00erent sources for Non technical Jobs using Beautiful Soup(Web\nScraping).\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "82d84f09-7d30-435f-839f-bf83e19b907b", "node_type": "4", "metadata": {}, "hash": "b75d4ee1f34ec4f633bd750cdc013c8465514c53ed5e8a2c488c15f028158ec9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0afe309c-44d9-40db-a58e-530a002c8b55", "node_type": "1", "metadata": {"window": "Applied both categorical cross-entropy and binary cross-entropy loss functions, yielding 85%\naccuracy for categorical loss and 55% for binary loss.\n \u2022Leveraged advanced LoRA techniques to \ufb01ne-tune LLM model, resulting in a remarkable 4%increase in accuracy\nwithin a mere 3training epochs.\n Interview Preparation Chatbot: |gtts , speech recognition , Chroma, DuckDB, Gemini.  Live |DagsHub\n\u2022Collaborated with a team of 30to develop a Chatbot web application for Interview preparation.\n \u2022Collected 7000 questions and answers from di\ufb00erent sources for Non technical Jobs using Beautiful Soup(Web\nScraping).\n \u2022Implemented Speech Recognition technology to enable seamless interaction, enhancing the user experience\nthrough voice driven commands and responses using gttsand speech recognition.\n \u2022Implemented RAG (Retrieval-Augmented Generation) with Chroma and DuckDB as vector databases for e\ufb03cient\nretrieval\n\u2022The answers were evaluated using the vector similarity check.\n", "original_text": "Live |DagsHub\n\u2022Collaborated with a team of 30to develop a Chatbot web application for Interview preparation.\n"}, "hash": "b9b48789b175b7ab6ebd47f933e41f77b40b7d9d5f4981778a56909df20a8754", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "37a49867-b891-4ec4-9a6f-0c3adb4b379b", "node_type": "1", "metadata": {"window": "Interview Preparation Chatbot: |gtts , speech recognition , Chroma, DuckDB, Gemini.  Live |DagsHub\n\u2022Collaborated with a team of 30to develop a Chatbot web application for Interview preparation.\n \u2022Collected 7000 questions and answers from di\ufb00erent sources for Non technical Jobs using Beautiful Soup(Web\nScraping).\n \u2022Implemented Speech Recognition technology to enable seamless interaction, enhancing the user experience\nthrough voice driven commands and responses using gttsand speech recognition.\n \u2022Implemented RAG (Retrieval-Augmented Generation) with Chroma and DuckDB as vector databases for e\ufb03cient\nretrieval\n\u2022The answers were evaluated using the vector similarity check.\n GemInsights: AI Automated Analyst |Autoviz, Gemini, llama index, Trulens Live |GitHub\n\u2022Collaborated with a team of 6during a Gemini hackathon organized by LabLab, developing an automatic EDA\n(Exploratory Data Analysis) application using Gemini models.\n \u2022Generated Insights from 7visualizations about the data and provide clarity in decision making\n\u2022Evaluated the answers from the LLMs using Trulens .\n", "original_text": "\u2022Implemented Speech Recognition technology to enable seamless interaction, enhancing the user experience\nthrough voice driven commands and responses using gttsand speech recognition.\n"}, "hash": "b2cb5bae61e89e1e1156b1a0bfb4ada0a52e92cdce1ed45ba4bcc5bcd0b36848", "class_name": "RelatedNodeInfo"}}, "text": "\u2022Collected 7000 questions and answers from di\ufb00erent sources for Non technical Jobs using Beautiful Soup(Web\nScraping).\n", "start_char_idx": 2381, "end_char_idx": 2500, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "37a49867-b891-4ec4-9a6f-0c3adb4b379b": {"__data__": {"id_": "37a49867-b891-4ec4-9a6f-0c3adb4b379b", "embedding": null, "metadata": {"window": "Interview Preparation Chatbot: |gtts , speech recognition , Chroma, DuckDB, Gemini.  Live |DagsHub\n\u2022Collaborated with a team of 30to develop a Chatbot web application for Interview preparation.\n \u2022Collected 7000 questions and answers from di\ufb00erent sources for Non technical Jobs using Beautiful Soup(Web\nScraping).\n \u2022Implemented Speech Recognition technology to enable seamless interaction, enhancing the user experience\nthrough voice driven commands and responses using gttsand speech recognition.\n \u2022Implemented RAG (Retrieval-Augmented Generation) with Chroma and DuckDB as vector databases for e\ufb03cient\nretrieval\n\u2022The answers were evaluated using the vector similarity check.\n GemInsights: AI Automated Analyst |Autoviz, Gemini, llama index, Trulens Live |GitHub\n\u2022Collaborated with a team of 6during a Gemini hackathon organized by LabLab, developing an automatic EDA\n(Exploratory Data Analysis) application using Gemini models.\n \u2022Generated Insights from 7visualizations about the data and provide clarity in decision making\n\u2022Evaluated the answers from the LLMs using Trulens .\n", "original_text": "\u2022Implemented Speech Recognition technology to enable seamless interaction, enhancing the user experience\nthrough voice driven commands and responses using gttsand speech recognition.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "82d84f09-7d30-435f-839f-bf83e19b907b", "node_type": "4", "metadata": {}, "hash": "b75d4ee1f34ec4f633bd750cdc013c8465514c53ed5e8a2c488c15f028158ec9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "79b1ee48-d786-4146-a49c-c444f5487301", "node_type": "1", "metadata": {"window": "\u2022Leveraged advanced LoRA techniques to \ufb01ne-tune LLM model, resulting in a remarkable 4%increase in accuracy\nwithin a mere 3training epochs.\n Interview Preparation Chatbot: |gtts , speech recognition , Chroma, DuckDB, Gemini.  Live |DagsHub\n\u2022Collaborated with a team of 30to develop a Chatbot web application for Interview preparation.\n \u2022Collected 7000 questions and answers from di\ufb00erent sources for Non technical Jobs using Beautiful Soup(Web\nScraping).\n \u2022Implemented Speech Recognition technology to enable seamless interaction, enhancing the user experience\nthrough voice driven commands and responses using gttsand speech recognition.\n \u2022Implemented RAG (Retrieval-Augmented Generation) with Chroma and DuckDB as vector databases for e\ufb03cient\nretrieval\n\u2022The answers were evaluated using the vector similarity check.\n GemInsights: AI Automated Analyst |Autoviz, Gemini, llama index, Trulens Live |GitHub\n\u2022Collaborated with a team of 6during a Gemini hackathon organized by LabLab, developing an automatic EDA\n(Exploratory Data Analysis) application using Gemini models.\n", "original_text": "\u2022Collected 7000 questions and answers from di\ufb00erent sources for Non technical Jobs using Beautiful Soup(Web\nScraping).\n"}, "hash": "5086d7e453d88fc57c2a24214f791e52582025903b56487f9b852195bad8fbb1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "51895717-bb4a-42ad-8db9-7f2438109c5b", "node_type": "1", "metadata": {"window": "Live |DagsHub\n\u2022Collaborated with a team of 30to develop a Chatbot web application for Interview preparation.\n \u2022Collected 7000 questions and answers from di\ufb00erent sources for Non technical Jobs using Beautiful Soup(Web\nScraping).\n \u2022Implemented Speech Recognition technology to enable seamless interaction, enhancing the user experience\nthrough voice driven commands and responses using gttsand speech recognition.\n \u2022Implemented RAG (Retrieval-Augmented Generation) with Chroma and DuckDB as vector databases for e\ufb03cient\nretrieval\n\u2022The answers were evaluated using the vector similarity check.\n GemInsights: AI Automated Analyst |Autoviz, Gemini, llama index, Trulens Live |GitHub\n\u2022Collaborated with a team of 6during a Gemini hackathon organized by LabLab, developing an automatic EDA\n(Exploratory Data Analysis) application using Gemini models.\n \u2022Generated Insights from 7visualizations about the data and provide clarity in decision making\n\u2022Evaluated the answers from the LLMs using Trulens .\n \u2022The project were selected as the most upvoted project in the Gemini Hackathon.\n", "original_text": "\u2022Implemented RAG (Retrieval-Augmented Generation) with Chroma and DuckDB as vector databases for e\ufb03cient\nretrieval\n\u2022The answers were evaluated using the vector similarity check.\n"}, "hash": "35542a4a922998662eca7a6efb095b37b3adc9fe9a48427fa2f5a3a70f10139c", "class_name": "RelatedNodeInfo"}}, "text": "\u2022Implemented Speech Recognition technology to enable seamless interaction, enhancing the user experience\nthrough voice driven commands and responses using gttsand speech recognition.\n", "start_char_idx": 2500, "end_char_idx": 2683, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "51895717-bb4a-42ad-8db9-7f2438109c5b": {"__data__": {"id_": "51895717-bb4a-42ad-8db9-7f2438109c5b", "embedding": null, "metadata": {"window": "Live |DagsHub\n\u2022Collaborated with a team of 30to develop a Chatbot web application for Interview preparation.\n \u2022Collected 7000 questions and answers from di\ufb00erent sources for Non technical Jobs using Beautiful Soup(Web\nScraping).\n \u2022Implemented Speech Recognition technology to enable seamless interaction, enhancing the user experience\nthrough voice driven commands and responses using gttsand speech recognition.\n \u2022Implemented RAG (Retrieval-Augmented Generation) with Chroma and DuckDB as vector databases for e\ufb03cient\nretrieval\n\u2022The answers were evaluated using the vector similarity check.\n GemInsights: AI Automated Analyst |Autoviz, Gemini, llama index, Trulens Live |GitHub\n\u2022Collaborated with a team of 6during a Gemini hackathon organized by LabLab, developing an automatic EDA\n(Exploratory Data Analysis) application using Gemini models.\n \u2022Generated Insights from 7visualizations about the data and provide clarity in decision making\n\u2022Evaluated the answers from the LLMs using Trulens .\n \u2022The project were selected as the most upvoted project in the Gemini Hackathon.\n", "original_text": "\u2022Implemented RAG (Retrieval-Augmented Generation) with Chroma and DuckDB as vector databases for e\ufb03cient\nretrieval\n\u2022The answers were evaluated using the vector similarity check.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "82d84f09-7d30-435f-839f-bf83e19b907b", "node_type": "4", "metadata": {}, "hash": "b75d4ee1f34ec4f633bd750cdc013c8465514c53ed5e8a2c488c15f028158ec9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "37a49867-b891-4ec4-9a6f-0c3adb4b379b", "node_type": "1", "metadata": {"window": "Interview Preparation Chatbot: |gtts , speech recognition , Chroma, DuckDB, Gemini.  Live |DagsHub\n\u2022Collaborated with a team of 30to develop a Chatbot web application for Interview preparation.\n \u2022Collected 7000 questions and answers from di\ufb00erent sources for Non technical Jobs using Beautiful Soup(Web\nScraping).\n \u2022Implemented Speech Recognition technology to enable seamless interaction, enhancing the user experience\nthrough voice driven commands and responses using gttsand speech recognition.\n \u2022Implemented RAG (Retrieval-Augmented Generation) with Chroma and DuckDB as vector databases for e\ufb03cient\nretrieval\n\u2022The answers were evaluated using the vector similarity check.\n GemInsights: AI Automated Analyst |Autoviz, Gemini, llama index, Trulens Live |GitHub\n\u2022Collaborated with a team of 6during a Gemini hackathon organized by LabLab, developing an automatic EDA\n(Exploratory Data Analysis) application using Gemini models.\n \u2022Generated Insights from 7visualizations about the data and provide clarity in decision making\n\u2022Evaluated the answers from the LLMs using Trulens .\n", "original_text": "\u2022Implemented Speech Recognition technology to enable seamless interaction, enhancing the user experience\nthrough voice driven commands and responses using gttsand speech recognition.\n"}, "hash": "b2cb5bae61e89e1e1156b1a0bfb4ada0a52e92cdce1ed45ba4bcc5bcd0b36848", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5fa511f5-e4bc-4f0c-ae6d-51ba33671408", "node_type": "1", "metadata": {"window": "\u2022Collected 7000 questions and answers from di\ufb00erent sources for Non technical Jobs using Beautiful Soup(Web\nScraping).\n \u2022Implemented Speech Recognition technology to enable seamless interaction, enhancing the user experience\nthrough voice driven commands and responses using gttsand speech recognition.\n \u2022Implemented RAG (Retrieval-Augmented Generation) with Chroma and DuckDB as vector databases for e\ufb03cient\nretrieval\n\u2022The answers were evaluated using the vector similarity check.\n GemInsights: AI Automated Analyst |Autoviz, Gemini, llama index, Trulens Live |GitHub\n\u2022Collaborated with a team of 6during a Gemini hackathon organized by LabLab, developing an automatic EDA\n(Exploratory Data Analysis) application using Gemini models.\n \u2022Generated Insights from 7visualizations about the data and provide clarity in decision making\n\u2022Evaluated the answers from the LLMs using Trulens .\n \u2022The project were selected as the most upvoted project in the Gemini Hackathon.\n Personal Projects\nGenEye: Your Mental Health Assistant |llama2, langchain , speech recognition, gtts, Qlora, Peft, Chroma Github\n\u2022Conversational Chatbot built Large Language Model llama2 and Langchain as Framework for visually impaired\npeople.\n", "original_text": "GemInsights: AI Automated Analyst |Autoviz, Gemini, llama index, Trulens Live |GitHub\n\u2022Collaborated with a team of 6during a Gemini hackathon organized by LabLab, developing an automatic EDA\n(Exploratory Data Analysis) application using Gemini models.\n"}, "hash": "33be1540aeedb8de4834af8ea6ff630cd0e5fd352b4b8064c792dd4bba52282d", "class_name": "RelatedNodeInfo"}}, "text": "\u2022Implemented RAG (Retrieval-Augmented Generation) with Chroma and DuckDB as vector databases for e\ufb03cient\nretrieval\n\u2022The answers were evaluated using the vector similarity check.\n", "start_char_idx": 2683, "end_char_idx": 2861, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5fa511f5-e4bc-4f0c-ae6d-51ba33671408": {"__data__": {"id_": "5fa511f5-e4bc-4f0c-ae6d-51ba33671408", "embedding": null, "metadata": {"window": "\u2022Collected 7000 questions and answers from di\ufb00erent sources for Non technical Jobs using Beautiful Soup(Web\nScraping).\n \u2022Implemented Speech Recognition technology to enable seamless interaction, enhancing the user experience\nthrough voice driven commands and responses using gttsand speech recognition.\n \u2022Implemented RAG (Retrieval-Augmented Generation) with Chroma and DuckDB as vector databases for e\ufb03cient\nretrieval\n\u2022The answers were evaluated using the vector similarity check.\n GemInsights: AI Automated Analyst |Autoviz, Gemini, llama index, Trulens Live |GitHub\n\u2022Collaborated with a team of 6during a Gemini hackathon organized by LabLab, developing an automatic EDA\n(Exploratory Data Analysis) application using Gemini models.\n \u2022Generated Insights from 7visualizations about the data and provide clarity in decision making\n\u2022Evaluated the answers from the LLMs using Trulens .\n \u2022The project were selected as the most upvoted project in the Gemini Hackathon.\n Personal Projects\nGenEye: Your Mental Health Assistant |llama2, langchain , speech recognition, gtts, Qlora, Peft, Chroma Github\n\u2022Conversational Chatbot built Large Language Model llama2 and Langchain as Framework for visually impaired\npeople.\n", "original_text": "GemInsights: AI Automated Analyst |Autoviz, Gemini, llama index, Trulens Live |GitHub\n\u2022Collaborated with a team of 6during a Gemini hackathon organized by LabLab, developing an automatic EDA\n(Exploratory Data Analysis) application using Gemini models.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "82d84f09-7d30-435f-839f-bf83e19b907b", "node_type": "4", "metadata": {}, "hash": "b75d4ee1f34ec4f633bd750cdc013c8465514c53ed5e8a2c488c15f028158ec9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "51895717-bb4a-42ad-8db9-7f2438109c5b", "node_type": "1", "metadata": {"window": "Live |DagsHub\n\u2022Collaborated with a team of 30to develop a Chatbot web application for Interview preparation.\n \u2022Collected 7000 questions and answers from di\ufb00erent sources for Non technical Jobs using Beautiful Soup(Web\nScraping).\n \u2022Implemented Speech Recognition technology to enable seamless interaction, enhancing the user experience\nthrough voice driven commands and responses using gttsand speech recognition.\n \u2022Implemented RAG (Retrieval-Augmented Generation) with Chroma and DuckDB as vector databases for e\ufb03cient\nretrieval\n\u2022The answers were evaluated using the vector similarity check.\n GemInsights: AI Automated Analyst |Autoviz, Gemini, llama index, Trulens Live |GitHub\n\u2022Collaborated with a team of 6during a Gemini hackathon organized by LabLab, developing an automatic EDA\n(Exploratory Data Analysis) application using Gemini models.\n \u2022Generated Insights from 7visualizations about the data and provide clarity in decision making\n\u2022Evaluated the answers from the LLMs using Trulens .\n \u2022The project were selected as the most upvoted project in the Gemini Hackathon.\n", "original_text": "\u2022Implemented RAG (Retrieval-Augmented Generation) with Chroma and DuckDB as vector databases for e\ufb03cient\nretrieval\n\u2022The answers were evaluated using the vector similarity check.\n"}, "hash": "35542a4a922998662eca7a6efb095b37b3adc9fe9a48427fa2f5a3a70f10139c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "36ddacfe-da6f-4060-b2c5-59200a90ef73", "node_type": "1", "metadata": {"window": "\u2022Implemented Speech Recognition technology to enable seamless interaction, enhancing the user experience\nthrough voice driven commands and responses using gttsand speech recognition.\n \u2022Implemented RAG (Retrieval-Augmented Generation) with Chroma and DuckDB as vector databases for e\ufb03cient\nretrieval\n\u2022The answers were evaluated using the vector similarity check.\n GemInsights: AI Automated Analyst |Autoviz, Gemini, llama index, Trulens Live |GitHub\n\u2022Collaborated with a team of 6during a Gemini hackathon organized by LabLab, developing an automatic EDA\n(Exploratory Data Analysis) application using Gemini models.\n \u2022Generated Insights from 7visualizations about the data and provide clarity in decision making\n\u2022Evaluated the answers from the LLMs using Trulens .\n \u2022The project were selected as the most upvoted project in the Gemini Hackathon.\n Personal Projects\nGenEye: Your Mental Health Assistant |llama2, langchain , speech recognition, gtts, Qlora, Peft, Chroma Github\n\u2022Conversational Chatbot built Large Language Model llama2 and Langchain as Framework for visually impaired\npeople.\n \u2022Used Libraries like speech recognition and gtts for identifying speech and converting the text to speech which can\nbe accessed by any type of people.\n", "original_text": "\u2022Generated Insights from 7visualizations about the data and provide clarity in decision making\n\u2022Evaluated the answers from the LLMs using Trulens .\n"}, "hash": "3bff012bedbbbb75f21e0f0f09890eb5bc3b7b409f24f11edbdd1eb9677b0554", "class_name": "RelatedNodeInfo"}}, "text": "GemInsights: AI Automated Analyst |Autoviz, Gemini, llama index, Trulens Live |GitHub\n\u2022Collaborated with a team of 6during a Gemini hackathon organized by LabLab, developing an automatic EDA\n(Exploratory Data Analysis) application using Gemini models.\n", "start_char_idx": 2861, "end_char_idx": 3113, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "36ddacfe-da6f-4060-b2c5-59200a90ef73": {"__data__": {"id_": "36ddacfe-da6f-4060-b2c5-59200a90ef73", "embedding": null, "metadata": {"window": "\u2022Implemented Speech Recognition technology to enable seamless interaction, enhancing the user experience\nthrough voice driven commands and responses using gttsand speech recognition.\n \u2022Implemented RAG (Retrieval-Augmented Generation) with Chroma and DuckDB as vector databases for e\ufb03cient\nretrieval\n\u2022The answers were evaluated using the vector similarity check.\n GemInsights: AI Automated Analyst |Autoviz, Gemini, llama index, Trulens Live |GitHub\n\u2022Collaborated with a team of 6during a Gemini hackathon organized by LabLab, developing an automatic EDA\n(Exploratory Data Analysis) application using Gemini models.\n \u2022Generated Insights from 7visualizations about the data and provide clarity in decision making\n\u2022Evaluated the answers from the LLMs using Trulens .\n \u2022The project were selected as the most upvoted project in the Gemini Hackathon.\n Personal Projects\nGenEye: Your Mental Health Assistant |llama2, langchain , speech recognition, gtts, Qlora, Peft, Chroma Github\n\u2022Conversational Chatbot built Large Language Model llama2 and Langchain as Framework for visually impaired\npeople.\n \u2022Used Libraries like speech recognition and gtts for identifying speech and converting the text to speech which can\nbe accessed by any type of people.\n", "original_text": "\u2022Generated Insights from 7visualizations about the data and provide clarity in decision making\n\u2022Evaluated the answers from the LLMs using Trulens .\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "82d84f09-7d30-435f-839f-bf83e19b907b", "node_type": "4", "metadata": {}, "hash": "b75d4ee1f34ec4f633bd750cdc013c8465514c53ed5e8a2c488c15f028158ec9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5fa511f5-e4bc-4f0c-ae6d-51ba33671408", "node_type": "1", "metadata": {"window": "\u2022Collected 7000 questions and answers from di\ufb00erent sources for Non technical Jobs using Beautiful Soup(Web\nScraping).\n \u2022Implemented Speech Recognition technology to enable seamless interaction, enhancing the user experience\nthrough voice driven commands and responses using gttsand speech recognition.\n \u2022Implemented RAG (Retrieval-Augmented Generation) with Chroma and DuckDB as vector databases for e\ufb03cient\nretrieval\n\u2022The answers were evaluated using the vector similarity check.\n GemInsights: AI Automated Analyst |Autoviz, Gemini, llama index, Trulens Live |GitHub\n\u2022Collaborated with a team of 6during a Gemini hackathon organized by LabLab, developing an automatic EDA\n(Exploratory Data Analysis) application using Gemini models.\n \u2022Generated Insights from 7visualizations about the data and provide clarity in decision making\n\u2022Evaluated the answers from the LLMs using Trulens .\n \u2022The project were selected as the most upvoted project in the Gemini Hackathon.\n Personal Projects\nGenEye: Your Mental Health Assistant |llama2, langchain , speech recognition, gtts, Qlora, Peft, Chroma Github\n\u2022Conversational Chatbot built Large Language Model llama2 and Langchain as Framework for visually impaired\npeople.\n", "original_text": "GemInsights: AI Automated Analyst |Autoviz, Gemini, llama index, Trulens Live |GitHub\n\u2022Collaborated with a team of 6during a Gemini hackathon organized by LabLab, developing an automatic EDA\n(Exploratory Data Analysis) application using Gemini models.\n"}, "hash": "33be1540aeedb8de4834af8ea6ff630cd0e5fd352b4b8064c792dd4bba52282d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b0f8925e-9d7a-44de-b385-ce1f0063ddc5", "node_type": "1", "metadata": {"window": "\u2022Implemented RAG (Retrieval-Augmented Generation) with Chroma and DuckDB as vector databases for e\ufb03cient\nretrieval\n\u2022The answers were evaluated using the vector similarity check.\n GemInsights: AI Automated Analyst |Autoviz, Gemini, llama index, Trulens Live |GitHub\n\u2022Collaborated with a team of 6during a Gemini hackathon organized by LabLab, developing an automatic EDA\n(Exploratory Data Analysis) application using Gemini models.\n \u2022Generated Insights from 7visualizations about the data and provide clarity in decision making\n\u2022Evaluated the answers from the LLMs using Trulens .\n \u2022The project were selected as the most upvoted project in the Gemini Hackathon.\n Personal Projects\nGenEye: Your Mental Health Assistant |llama2, langchain , speech recognition, gtts, Qlora, Peft, Chroma Github\n\u2022Conversational Chatbot built Large Language Model llama2 and Langchain as Framework for visually impaired\npeople.\n \u2022Used Libraries like speech recognition and gtts for identifying speech and converting the text to speech which can\nbe accessed by any type of people.\n \u2022Model \ufb01ne-tuned on Mental Health Dataset using Qlora and Peft.\n", "original_text": "\u2022The project were selected as the most upvoted project in the Gemini Hackathon.\n"}, "hash": "527d357dc62747895599160a29b54a359c234442300859774a3004704e9e11b4", "class_name": "RelatedNodeInfo"}}, "text": "\u2022Generated Insights from 7visualizations about the data and provide clarity in decision making\n\u2022Evaluated the answers from the LLMs using Trulens .\n", "start_char_idx": 3113, "end_char_idx": 3261, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b0f8925e-9d7a-44de-b385-ce1f0063ddc5": {"__data__": {"id_": "b0f8925e-9d7a-44de-b385-ce1f0063ddc5", "embedding": null, "metadata": {"window": "\u2022Implemented RAG (Retrieval-Augmented Generation) with Chroma and DuckDB as vector databases for e\ufb03cient\nretrieval\n\u2022The answers were evaluated using the vector similarity check.\n GemInsights: AI Automated Analyst |Autoviz, Gemini, llama index, Trulens Live |GitHub\n\u2022Collaborated with a team of 6during a Gemini hackathon organized by LabLab, developing an automatic EDA\n(Exploratory Data Analysis) application using Gemini models.\n \u2022Generated Insights from 7visualizations about the data and provide clarity in decision making\n\u2022Evaluated the answers from the LLMs using Trulens .\n \u2022The project were selected as the most upvoted project in the Gemini Hackathon.\n Personal Projects\nGenEye: Your Mental Health Assistant |llama2, langchain , speech recognition, gtts, Qlora, Peft, Chroma Github\n\u2022Conversational Chatbot built Large Language Model llama2 and Langchain as Framework for visually impaired\npeople.\n \u2022Used Libraries like speech recognition and gtts for identifying speech and converting the text to speech which can\nbe accessed by any type of people.\n \u2022Model \ufb01ne-tuned on Mental Health Dataset using Qlora and Peft.\n", "original_text": "\u2022The project were selected as the most upvoted project in the Gemini Hackathon.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "82d84f09-7d30-435f-839f-bf83e19b907b", "node_type": "4", "metadata": {}, "hash": "b75d4ee1f34ec4f633bd750cdc013c8465514c53ed5e8a2c488c15f028158ec9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "36ddacfe-da6f-4060-b2c5-59200a90ef73", "node_type": "1", "metadata": {"window": "\u2022Implemented Speech Recognition technology to enable seamless interaction, enhancing the user experience\nthrough voice driven commands and responses using gttsand speech recognition.\n \u2022Implemented RAG (Retrieval-Augmented Generation) with Chroma and DuckDB as vector databases for e\ufb03cient\nretrieval\n\u2022The answers were evaluated using the vector similarity check.\n GemInsights: AI Automated Analyst |Autoviz, Gemini, llama index, Trulens Live |GitHub\n\u2022Collaborated with a team of 6during a Gemini hackathon organized by LabLab, developing an automatic EDA\n(Exploratory Data Analysis) application using Gemini models.\n \u2022Generated Insights from 7visualizations about the data and provide clarity in decision making\n\u2022Evaluated the answers from the LLMs using Trulens .\n \u2022The project were selected as the most upvoted project in the Gemini Hackathon.\n Personal Projects\nGenEye: Your Mental Health Assistant |llama2, langchain , speech recognition, gtts, Qlora, Peft, Chroma Github\n\u2022Conversational Chatbot built Large Language Model llama2 and Langchain as Framework for visually impaired\npeople.\n \u2022Used Libraries like speech recognition and gtts for identifying speech and converting the text to speech which can\nbe accessed by any type of people.\n", "original_text": "\u2022Generated Insights from 7visualizations about the data and provide clarity in decision making\n\u2022Evaluated the answers from the LLMs using Trulens .\n"}, "hash": "3bff012bedbbbb75f21e0f0f09890eb5bc3b7b409f24f11edbdd1eb9677b0554", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "08123b7d-7164-4e6b-b9d4-d80b73fd7272", "node_type": "1", "metadata": {"window": "GemInsights: AI Automated Analyst |Autoviz, Gemini, llama index, Trulens Live |GitHub\n\u2022Collaborated with a team of 6during a Gemini hackathon organized by LabLab, developing an automatic EDA\n(Exploratory Data Analysis) application using Gemini models.\n \u2022Generated Insights from 7visualizations about the data and provide clarity in decision making\n\u2022Evaluated the answers from the LLMs using Trulens .\n \u2022The project were selected as the most upvoted project in the Gemini Hackathon.\n Personal Projects\nGenEye: Your Mental Health Assistant |llama2, langchain , speech recognition, gtts, Qlora, Peft, Chroma Github\n\u2022Conversational Chatbot built Large Language Model llama2 and Langchain as Framework for visually impaired\npeople.\n \u2022Used Libraries like speech recognition and gtts for identifying speech and converting the text to speech which can\nbe accessed by any type of people.\n \u2022Model \ufb01ne-tuned on Mental Health Dataset using Qlora and Peft.\n Road Accident Dashboard |Tableau Link\n\u2022Developed a comprehensive road accident dashboard using Tableau to analyze 2019 - 2022 accident data and\nimprove decision-making processes.\n", "original_text": "Personal Projects\nGenEye: Your Mental Health Assistant |llama2, langchain , speech recognition, gtts, Qlora, Peft, Chroma Github\n\u2022Conversational Chatbot built Large Language Model llama2 and Langchain as Framework for visually impaired\npeople.\n"}, "hash": "1b981e305102bdf0eaa900c64478e30a2e2a8d09471d9269ae07749d8a4f99ac", "class_name": "RelatedNodeInfo"}}, "text": "\u2022The project were selected as the most upvoted project in the Gemini Hackathon.\n", "start_char_idx": 3261, "end_char_idx": 3341, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "08123b7d-7164-4e6b-b9d4-d80b73fd7272": {"__data__": {"id_": "08123b7d-7164-4e6b-b9d4-d80b73fd7272", "embedding": null, "metadata": {"window": "GemInsights: AI Automated Analyst |Autoviz, Gemini, llama index, Trulens Live |GitHub\n\u2022Collaborated with a team of 6during a Gemini hackathon organized by LabLab, developing an automatic EDA\n(Exploratory Data Analysis) application using Gemini models.\n \u2022Generated Insights from 7visualizations about the data and provide clarity in decision making\n\u2022Evaluated the answers from the LLMs using Trulens .\n \u2022The project were selected as the most upvoted project in the Gemini Hackathon.\n Personal Projects\nGenEye: Your Mental Health Assistant |llama2, langchain , speech recognition, gtts, Qlora, Peft, Chroma Github\n\u2022Conversational Chatbot built Large Language Model llama2 and Langchain as Framework for visually impaired\npeople.\n \u2022Used Libraries like speech recognition and gtts for identifying speech and converting the text to speech which can\nbe accessed by any type of people.\n \u2022Model \ufb01ne-tuned on Mental Health Dataset using Qlora and Peft.\n Road Accident Dashboard |Tableau Link\n\u2022Developed a comprehensive road accident dashboard using Tableau to analyze 2019 - 2022 accident data and\nimprove decision-making processes.\n", "original_text": "Personal Projects\nGenEye: Your Mental Health Assistant |llama2, langchain , speech recognition, gtts, Qlora, Peft, Chroma Github\n\u2022Conversational Chatbot built Large Language Model llama2 and Langchain as Framework for visually impaired\npeople.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "82d84f09-7d30-435f-839f-bf83e19b907b", "node_type": "4", "metadata": {}, "hash": "b75d4ee1f34ec4f633bd750cdc013c8465514c53ed5e8a2c488c15f028158ec9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b0f8925e-9d7a-44de-b385-ce1f0063ddc5", "node_type": "1", "metadata": {"window": "\u2022Implemented RAG (Retrieval-Augmented Generation) with Chroma and DuckDB as vector databases for e\ufb03cient\nretrieval\n\u2022The answers were evaluated using the vector similarity check.\n GemInsights: AI Automated Analyst |Autoviz, Gemini, llama index, Trulens Live |GitHub\n\u2022Collaborated with a team of 6during a Gemini hackathon organized by LabLab, developing an automatic EDA\n(Exploratory Data Analysis) application using Gemini models.\n \u2022Generated Insights from 7visualizations about the data and provide clarity in decision making\n\u2022Evaluated the answers from the LLMs using Trulens .\n \u2022The project were selected as the most upvoted project in the Gemini Hackathon.\n Personal Projects\nGenEye: Your Mental Health Assistant |llama2, langchain , speech recognition, gtts, Qlora, Peft, Chroma Github\n\u2022Conversational Chatbot built Large Language Model llama2 and Langchain as Framework for visually impaired\npeople.\n \u2022Used Libraries like speech recognition and gtts for identifying speech and converting the text to speech which can\nbe accessed by any type of people.\n \u2022Model \ufb01ne-tuned on Mental Health Dataset using Qlora and Peft.\n", "original_text": "\u2022The project were selected as the most upvoted project in the Gemini Hackathon.\n"}, "hash": "527d357dc62747895599160a29b54a359c234442300859774a3004704e9e11b4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6feb1a13-9b49-4c67-820a-f7e502d49aa1", "node_type": "1", "metadata": {"window": "\u2022Generated Insights from 7visualizations about the data and provide clarity in decision making\n\u2022Evaluated the answers from the LLMs using Trulens .\n \u2022The project were selected as the most upvoted project in the Gemini Hackathon.\n Personal Projects\nGenEye: Your Mental Health Assistant |llama2, langchain , speech recognition, gtts, Qlora, Peft, Chroma Github\n\u2022Conversational Chatbot built Large Language Model llama2 and Langchain as Framework for visually impaired\npeople.\n \u2022Used Libraries like speech recognition and gtts for identifying speech and converting the text to speech which can\nbe accessed by any type of people.\n \u2022Model \ufb01ne-tuned on Mental Health Dataset using Qlora and Peft.\n Road Accident Dashboard |Tableau Link\n\u2022Developed a comprehensive road accident dashboard using Tableau to analyze 2019 - 2022 accident data and\nimprove decision-making processes.\n \u2022Categorized accidents by 3 levels of severity and used visualizations to delve into the factors contributing to severe\naccidents.\n", "original_text": "\u2022Used Libraries like speech recognition and gtts for identifying speech and converting the text to speech which can\nbe accessed by any type of people.\n"}, "hash": "a808262ac6fb3c76a7ada3489d4667822fe75401dd1939358c09b5010378ba3f", "class_name": "RelatedNodeInfo"}}, "text": "Personal Projects\nGenEye: Your Mental Health Assistant |llama2, langchain , speech recognition, gtts, Qlora, Peft, Chroma Github\n\u2022Conversational Chatbot built Large Language Model llama2 and Langchain as Framework for visually impaired\npeople.\n", "start_char_idx": 3341, "end_char_idx": 3585, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6feb1a13-9b49-4c67-820a-f7e502d49aa1": {"__data__": {"id_": "6feb1a13-9b49-4c67-820a-f7e502d49aa1", "embedding": null, "metadata": {"window": "\u2022Generated Insights from 7visualizations about the data and provide clarity in decision making\n\u2022Evaluated the answers from the LLMs using Trulens .\n \u2022The project were selected as the most upvoted project in the Gemini Hackathon.\n Personal Projects\nGenEye: Your Mental Health Assistant |llama2, langchain , speech recognition, gtts, Qlora, Peft, Chroma Github\n\u2022Conversational Chatbot built Large Language Model llama2 and Langchain as Framework for visually impaired\npeople.\n \u2022Used Libraries like speech recognition and gtts for identifying speech and converting the text to speech which can\nbe accessed by any type of people.\n \u2022Model \ufb01ne-tuned on Mental Health Dataset using Qlora and Peft.\n Road Accident Dashboard |Tableau Link\n\u2022Developed a comprehensive road accident dashboard using Tableau to analyze 2019 - 2022 accident data and\nimprove decision-making processes.\n \u2022Categorized accidents by 3 levels of severity and used visualizations to delve into the factors contributing to severe\naccidents.\n", "original_text": "\u2022Used Libraries like speech recognition and gtts for identifying speech and converting the text to speech which can\nbe accessed by any type of people.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "82d84f09-7d30-435f-839f-bf83e19b907b", "node_type": "4", "metadata": {}, "hash": "b75d4ee1f34ec4f633bd750cdc013c8465514c53ed5e8a2c488c15f028158ec9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "08123b7d-7164-4e6b-b9d4-d80b73fd7272", "node_type": "1", "metadata": {"window": "GemInsights: AI Automated Analyst |Autoviz, Gemini, llama index, Trulens Live |GitHub\n\u2022Collaborated with a team of 6during a Gemini hackathon organized by LabLab, developing an automatic EDA\n(Exploratory Data Analysis) application using Gemini models.\n \u2022Generated Insights from 7visualizations about the data and provide clarity in decision making\n\u2022Evaluated the answers from the LLMs using Trulens .\n \u2022The project were selected as the most upvoted project in the Gemini Hackathon.\n Personal Projects\nGenEye: Your Mental Health Assistant |llama2, langchain , speech recognition, gtts, Qlora, Peft, Chroma Github\n\u2022Conversational Chatbot built Large Language Model llama2 and Langchain as Framework for visually impaired\npeople.\n \u2022Used Libraries like speech recognition and gtts for identifying speech and converting the text to speech which can\nbe accessed by any type of people.\n \u2022Model \ufb01ne-tuned on Mental Health Dataset using Qlora and Peft.\n Road Accident Dashboard |Tableau Link\n\u2022Developed a comprehensive road accident dashboard using Tableau to analyze 2019 - 2022 accident data and\nimprove decision-making processes.\n", "original_text": "Personal Projects\nGenEye: Your Mental Health Assistant |llama2, langchain , speech recognition, gtts, Qlora, Peft, Chroma Github\n\u2022Conversational Chatbot built Large Language Model llama2 and Langchain as Framework for visually impaired\npeople.\n"}, "hash": "1b981e305102bdf0eaa900c64478e30a2e2a8d09471d9269ae07749d8a4f99ac", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4a978267-d802-40cf-98d3-04cfaef37695", "node_type": "1", "metadata": {"window": "\u2022The project were selected as the most upvoted project in the Gemini Hackathon.\n Personal Projects\nGenEye: Your Mental Health Assistant |llama2, langchain , speech recognition, gtts, Qlora, Peft, Chroma Github\n\u2022Conversational Chatbot built Large Language Model llama2 and Langchain as Framework for visually impaired\npeople.\n \u2022Used Libraries like speech recognition and gtts for identifying speech and converting the text to speech which can\nbe accessed by any type of people.\n \u2022Model \ufb01ne-tuned on Mental Health Dataset using Qlora and Peft.\n Road Accident Dashboard |Tableau Link\n\u2022Developed a comprehensive road accident dashboard using Tableau to analyze 2019 - 2022 accident data and\nimprove decision-making processes.\n \u2022Categorized accidents by 3 levels of severity and used visualizations to delve into the factors contributing to severe\naccidents.\n \u2022Conducted comparative analyses of accident statistics across regions and time periods, facilitating the assessment of\nsafety measures and interventions.\n", "original_text": "\u2022Model \ufb01ne-tuned on Mental Health Dataset using Qlora and Peft.\n"}, "hash": "d0685c0b62e6421de77102f9f04f6c9fbfb4cd923e4df4f2b81862bf5bb98d70", "class_name": "RelatedNodeInfo"}}, "text": "\u2022Used Libraries like speech recognition and gtts for identifying speech and converting the text to speech which can\nbe accessed by any type of people.\n", "start_char_idx": 3585, "end_char_idx": 3736, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4a978267-d802-40cf-98d3-04cfaef37695": {"__data__": {"id_": "4a978267-d802-40cf-98d3-04cfaef37695", "embedding": null, "metadata": {"window": "\u2022The project were selected as the most upvoted project in the Gemini Hackathon.\n Personal Projects\nGenEye: Your Mental Health Assistant |llama2, langchain , speech recognition, gtts, Qlora, Peft, Chroma Github\n\u2022Conversational Chatbot built Large Language Model llama2 and Langchain as Framework for visually impaired\npeople.\n \u2022Used Libraries like speech recognition and gtts for identifying speech and converting the text to speech which can\nbe accessed by any type of people.\n \u2022Model \ufb01ne-tuned on Mental Health Dataset using Qlora and Peft.\n Road Accident Dashboard |Tableau Link\n\u2022Developed a comprehensive road accident dashboard using Tableau to analyze 2019 - 2022 accident data and\nimprove decision-making processes.\n \u2022Categorized accidents by 3 levels of severity and used visualizations to delve into the factors contributing to severe\naccidents.\n \u2022Conducted comparative analyses of accident statistics across regions and time periods, facilitating the assessment of\nsafety measures and interventions.\n", "original_text": "\u2022Model \ufb01ne-tuned on Mental Health Dataset using Qlora and Peft.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "82d84f09-7d30-435f-839f-bf83e19b907b", "node_type": "4", "metadata": {}, "hash": "b75d4ee1f34ec4f633bd750cdc013c8465514c53ed5e8a2c488c15f028158ec9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6feb1a13-9b49-4c67-820a-f7e502d49aa1", "node_type": "1", "metadata": {"window": "\u2022Generated Insights from 7visualizations about the data and provide clarity in decision making\n\u2022Evaluated the answers from the LLMs using Trulens .\n \u2022The project were selected as the most upvoted project in the Gemini Hackathon.\n Personal Projects\nGenEye: Your Mental Health Assistant |llama2, langchain , speech recognition, gtts, Qlora, Peft, Chroma Github\n\u2022Conversational Chatbot built Large Language Model llama2 and Langchain as Framework for visually impaired\npeople.\n \u2022Used Libraries like speech recognition and gtts for identifying speech and converting the text to speech which can\nbe accessed by any type of people.\n \u2022Model \ufb01ne-tuned on Mental Health Dataset using Qlora and Peft.\n Road Accident Dashboard |Tableau Link\n\u2022Developed a comprehensive road accident dashboard using Tableau to analyze 2019 - 2022 accident data and\nimprove decision-making processes.\n \u2022Categorized accidents by 3 levels of severity and used visualizations to delve into the factors contributing to severe\naccidents.\n", "original_text": "\u2022Used Libraries like speech recognition and gtts for identifying speech and converting the text to speech which can\nbe accessed by any type of people.\n"}, "hash": "a808262ac6fb3c76a7ada3489d4667822fe75401dd1939358c09b5010378ba3f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1c7c8928-6622-45f1-b2c4-02d5e5e0999c", "node_type": "1", "metadata": {"window": "Personal Projects\nGenEye: Your Mental Health Assistant |llama2, langchain , speech recognition, gtts, Qlora, Peft, Chroma Github\n\u2022Conversational Chatbot built Large Language Model llama2 and Langchain as Framework for visually impaired\npeople.\n \u2022Used Libraries like speech recognition and gtts for identifying speech and converting the text to speech which can\nbe accessed by any type of people.\n \u2022Model \ufb01ne-tuned on Mental Health Dataset using Qlora and Peft.\n Road Accident Dashboard |Tableau Link\n\u2022Developed a comprehensive road accident dashboard using Tableau to analyze 2019 - 2022 accident data and\nimprove decision-making processes.\n \u2022Categorized accidents by 3 levels of severity and used visualizations to delve into the factors contributing to severe\naccidents.\n \u2022Conducted comparative analyses of accident statistics across regions and time periods, facilitating the assessment of\nsafety measures and interventions.\n Experience\nOmdena Nov 2023 \u2013 Present\nJunior Machine Learning Engineer Remote\n\u2022Collaborated with 25+ AI Engineers, developers and clients in remote settings, ensuring seamless project execution\nand delivery of AI applications or products which solves real word problems.\n", "original_text": "Road Accident Dashboard |Tableau Link\n\u2022Developed a comprehensive road accident dashboard using Tableau to analyze 2019 - 2022 accident data and\nimprove decision-making processes.\n"}, "hash": "c3a3387d9f9320f18350da6a6f8ab3d88152b50d7c5bc2e7a27e28f84a7afef7", "class_name": "RelatedNodeInfo"}}, "text": "\u2022Model \ufb01ne-tuned on Mental Health Dataset using Qlora and Peft.\n", "start_char_idx": 3736, "end_char_idx": 3800, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1c7c8928-6622-45f1-b2c4-02d5e5e0999c": {"__data__": {"id_": "1c7c8928-6622-45f1-b2c4-02d5e5e0999c", "embedding": null, "metadata": {"window": "Personal Projects\nGenEye: Your Mental Health Assistant |llama2, langchain , speech recognition, gtts, Qlora, Peft, Chroma Github\n\u2022Conversational Chatbot built Large Language Model llama2 and Langchain as Framework for visually impaired\npeople.\n \u2022Used Libraries like speech recognition and gtts for identifying speech and converting the text to speech which can\nbe accessed by any type of people.\n \u2022Model \ufb01ne-tuned on Mental Health Dataset using Qlora and Peft.\n Road Accident Dashboard |Tableau Link\n\u2022Developed a comprehensive road accident dashboard using Tableau to analyze 2019 - 2022 accident data and\nimprove decision-making processes.\n \u2022Categorized accidents by 3 levels of severity and used visualizations to delve into the factors contributing to severe\naccidents.\n \u2022Conducted comparative analyses of accident statistics across regions and time periods, facilitating the assessment of\nsafety measures and interventions.\n Experience\nOmdena Nov 2023 \u2013 Present\nJunior Machine Learning Engineer Remote\n\u2022Collaborated with 25+ AI Engineers, developers and clients in remote settings, ensuring seamless project execution\nand delivery of AI applications or products which solves real word problems.\n", "original_text": "Road Accident Dashboard |Tableau Link\n\u2022Developed a comprehensive road accident dashboard using Tableau to analyze 2019 - 2022 accident data and\nimprove decision-making processes.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "82d84f09-7d30-435f-839f-bf83e19b907b", "node_type": "4", "metadata": {}, "hash": "b75d4ee1f34ec4f633bd750cdc013c8465514c53ed5e8a2c488c15f028158ec9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4a978267-d802-40cf-98d3-04cfaef37695", "node_type": "1", "metadata": {"window": "\u2022The project were selected as the most upvoted project in the Gemini Hackathon.\n Personal Projects\nGenEye: Your Mental Health Assistant |llama2, langchain , speech recognition, gtts, Qlora, Peft, Chroma Github\n\u2022Conversational Chatbot built Large Language Model llama2 and Langchain as Framework for visually impaired\npeople.\n \u2022Used Libraries like speech recognition and gtts for identifying speech and converting the text to speech which can\nbe accessed by any type of people.\n \u2022Model \ufb01ne-tuned on Mental Health Dataset using Qlora and Peft.\n Road Accident Dashboard |Tableau Link\n\u2022Developed a comprehensive road accident dashboard using Tableau to analyze 2019 - 2022 accident data and\nimprove decision-making processes.\n \u2022Categorized accidents by 3 levels of severity and used visualizations to delve into the factors contributing to severe\naccidents.\n \u2022Conducted comparative analyses of accident statistics across regions and time periods, facilitating the assessment of\nsafety measures and interventions.\n", "original_text": "\u2022Model \ufb01ne-tuned on Mental Health Dataset using Qlora and Peft.\n"}, "hash": "d0685c0b62e6421de77102f9f04f6c9fbfb4cd923e4df4f2b81862bf5bb98d70", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "308937ff-6ad0-4cbb-96a9-35b8a962caa0", "node_type": "1", "metadata": {"window": "\u2022Used Libraries like speech recognition and gtts for identifying speech and converting the text to speech which can\nbe accessed by any type of people.\n \u2022Model \ufb01ne-tuned on Mental Health Dataset using Qlora and Peft.\n Road Accident Dashboard |Tableau Link\n\u2022Developed a comprehensive road accident dashboard using Tableau to analyze 2019 - 2022 accident data and\nimprove decision-making processes.\n \u2022Categorized accidents by 3 levels of severity and used visualizations to delve into the factors contributing to severe\naccidents.\n \u2022Conducted comparative analyses of accident statistics across regions and time periods, facilitating the assessment of\nsafety measures and interventions.\n Experience\nOmdena Nov 2023 \u2013 Present\nJunior Machine Learning Engineer Remote\n\u2022Collaborated with 25+ AI Engineers, developers and clients in remote settings, ensuring seamless project execution\nand delivery of AI applications or products which solves real word problems.\n \u2022Done web scraping using beautiful soup and Apify to collect 100,000 data from di\ufb00erent sources to create dataset\nfor quality prediction.\n", "original_text": "\u2022Categorized accidents by 3 levels of severity and used visualizations to delve into the factors contributing to severe\naccidents.\n"}, "hash": "b4bed570ec8a2dde770bbec0b279045c50200d9c6cb6c0edee006e050245f4de", "class_name": "RelatedNodeInfo"}}, "text": "Road Accident Dashboard |Tableau Link\n\u2022Developed a comprehensive road accident dashboard using Tableau to analyze 2019 - 2022 accident data and\nimprove decision-making processes.\n", "start_char_idx": 3800, "end_char_idx": 3979, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "308937ff-6ad0-4cbb-96a9-35b8a962caa0": {"__data__": {"id_": "308937ff-6ad0-4cbb-96a9-35b8a962caa0", "embedding": null, "metadata": {"window": "\u2022Used Libraries like speech recognition and gtts for identifying speech and converting the text to speech which can\nbe accessed by any type of people.\n \u2022Model \ufb01ne-tuned on Mental Health Dataset using Qlora and Peft.\n Road Accident Dashboard |Tableau Link\n\u2022Developed a comprehensive road accident dashboard using Tableau to analyze 2019 - 2022 accident data and\nimprove decision-making processes.\n \u2022Categorized accidents by 3 levels of severity and used visualizations to delve into the factors contributing to severe\naccidents.\n \u2022Conducted comparative analyses of accident statistics across regions and time periods, facilitating the assessment of\nsafety measures and interventions.\n Experience\nOmdena Nov 2023 \u2013 Present\nJunior Machine Learning Engineer Remote\n\u2022Collaborated with 25+ AI Engineers, developers and clients in remote settings, ensuring seamless project execution\nand delivery of AI applications or products which solves real word problems.\n \u2022Done web scraping using beautiful soup and Apify to collect 100,000 data from di\ufb00erent sources to create dataset\nfor quality prediction.\n", "original_text": "\u2022Categorized accidents by 3 levels of severity and used visualizations to delve into the factors contributing to severe\naccidents.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "82d84f09-7d30-435f-839f-bf83e19b907b", "node_type": "4", "metadata": {}, "hash": "b75d4ee1f34ec4f633bd750cdc013c8465514c53ed5e8a2c488c15f028158ec9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1c7c8928-6622-45f1-b2c4-02d5e5e0999c", "node_type": "1", "metadata": {"window": "Personal Projects\nGenEye: Your Mental Health Assistant |llama2, langchain , speech recognition, gtts, Qlora, Peft, Chroma Github\n\u2022Conversational Chatbot built Large Language Model llama2 and Langchain as Framework for visually impaired\npeople.\n \u2022Used Libraries like speech recognition and gtts for identifying speech and converting the text to speech which can\nbe accessed by any type of people.\n \u2022Model \ufb01ne-tuned on Mental Health Dataset using Qlora and Peft.\n Road Accident Dashboard |Tableau Link\n\u2022Developed a comprehensive road accident dashboard using Tableau to analyze 2019 - 2022 accident data and\nimprove decision-making processes.\n \u2022Categorized accidents by 3 levels of severity and used visualizations to delve into the factors contributing to severe\naccidents.\n \u2022Conducted comparative analyses of accident statistics across regions and time periods, facilitating the assessment of\nsafety measures and interventions.\n Experience\nOmdena Nov 2023 \u2013 Present\nJunior Machine Learning Engineer Remote\n\u2022Collaborated with 25+ AI Engineers, developers and clients in remote settings, ensuring seamless project execution\nand delivery of AI applications or products which solves real word problems.\n", "original_text": "Road Accident Dashboard |Tableau Link\n\u2022Developed a comprehensive road accident dashboard using Tableau to analyze 2019 - 2022 accident data and\nimprove decision-making processes.\n"}, "hash": "c3a3387d9f9320f18350da6a6f8ab3d88152b50d7c5bc2e7a27e28f84a7afef7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "da7b9481-f7d5-4857-a91b-963cc0480ddf", "node_type": "1", "metadata": {"window": "\u2022Model \ufb01ne-tuned on Mental Health Dataset using Qlora and Peft.\n Road Accident Dashboard |Tableau Link\n\u2022Developed a comprehensive road accident dashboard using Tableau to analyze 2019 - 2022 accident data and\nimprove decision-making processes.\n \u2022Categorized accidents by 3 levels of severity and used visualizations to delve into the factors contributing to severe\naccidents.\n \u2022Conducted comparative analyses of accident statistics across regions and time periods, facilitating the assessment of\nsafety measures and interventions.\n Experience\nOmdena Nov 2023 \u2013 Present\nJunior Machine Learning Engineer Remote\n\u2022Collaborated with 25+ AI Engineers, developers and clients in remote settings, ensuring seamless project execution\nand delivery of AI applications or products which solves real word problems.\n \u2022Done web scraping using beautiful soup and Apify to collect 100,000 data from di\ufb00erent sources to create dataset\nfor quality prediction.\n \u2022Used Hugging face models like facebook/bart-large-mnli (zero-shot classi\ufb01cation ) and used language translation\nmodels like T-5,Bert and Deberta llm.\n", "original_text": "\u2022Conducted comparative analyses of accident statistics across regions and time periods, facilitating the assessment of\nsafety measures and interventions.\n"}, "hash": "c9e18d0fdb1df70c017195270c0531364730cb99d7927e02baae495f42e97874", "class_name": "RelatedNodeInfo"}}, "text": "\u2022Categorized accidents by 3 levels of severity and used visualizations to delve into the factors contributing to severe\naccidents.\n", "start_char_idx": 3979, "end_char_idx": 4110, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "da7b9481-f7d5-4857-a91b-963cc0480ddf": {"__data__": {"id_": "da7b9481-f7d5-4857-a91b-963cc0480ddf", "embedding": null, "metadata": {"window": "\u2022Model \ufb01ne-tuned on Mental Health Dataset using Qlora and Peft.\n Road Accident Dashboard |Tableau Link\n\u2022Developed a comprehensive road accident dashboard using Tableau to analyze 2019 - 2022 accident data and\nimprove decision-making processes.\n \u2022Categorized accidents by 3 levels of severity and used visualizations to delve into the factors contributing to severe\naccidents.\n \u2022Conducted comparative analyses of accident statistics across regions and time periods, facilitating the assessment of\nsafety measures and interventions.\n Experience\nOmdena Nov 2023 \u2013 Present\nJunior Machine Learning Engineer Remote\n\u2022Collaborated with 25+ AI Engineers, developers and clients in remote settings, ensuring seamless project execution\nand delivery of AI applications or products which solves real word problems.\n \u2022Done web scraping using beautiful soup and Apify to collect 100,000 data from di\ufb00erent sources to create dataset\nfor quality prediction.\n \u2022Used Hugging face models like facebook/bart-large-mnli (zero-shot classi\ufb01cation ) and used language translation\nmodels like T-5,Bert and Deberta llm.\n", "original_text": "\u2022Conducted comparative analyses of accident statistics across regions and time periods, facilitating the assessment of\nsafety measures and interventions.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "82d84f09-7d30-435f-839f-bf83e19b907b", "node_type": "4", "metadata": {}, "hash": "b75d4ee1f34ec4f633bd750cdc013c8465514c53ed5e8a2c488c15f028158ec9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "308937ff-6ad0-4cbb-96a9-35b8a962caa0", "node_type": "1", "metadata": {"window": "\u2022Used Libraries like speech recognition and gtts for identifying speech and converting the text to speech which can\nbe accessed by any type of people.\n \u2022Model \ufb01ne-tuned on Mental Health Dataset using Qlora and Peft.\n Road Accident Dashboard |Tableau Link\n\u2022Developed a comprehensive road accident dashboard using Tableau to analyze 2019 - 2022 accident data and\nimprove decision-making processes.\n \u2022Categorized accidents by 3 levels of severity and used visualizations to delve into the factors contributing to severe\naccidents.\n \u2022Conducted comparative analyses of accident statistics across regions and time periods, facilitating the assessment of\nsafety measures and interventions.\n Experience\nOmdena Nov 2023 \u2013 Present\nJunior Machine Learning Engineer Remote\n\u2022Collaborated with 25+ AI Engineers, developers and clients in remote settings, ensuring seamless project execution\nand delivery of AI applications or products which solves real word problems.\n \u2022Done web scraping using beautiful soup and Apify to collect 100,000 data from di\ufb00erent sources to create dataset\nfor quality prediction.\n", "original_text": "\u2022Categorized accidents by 3 levels of severity and used visualizations to delve into the factors contributing to severe\naccidents.\n"}, "hash": "b4bed570ec8a2dde770bbec0b279045c50200d9c6cb6c0edee006e050245f4de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "3cf3ca8e-3b4c-4beb-9ff2-e80403c73a90", "node_type": "1", "metadata": {"window": "Road Accident Dashboard |Tableau Link\n\u2022Developed a comprehensive road accident dashboard using Tableau to analyze 2019 - 2022 accident data and\nimprove decision-making processes.\n \u2022Categorized accidents by 3 levels of severity and used visualizations to delve into the factors contributing to severe\naccidents.\n \u2022Conducted comparative analyses of accident statistics across regions and time periods, facilitating the assessment of\nsafety measures and interventions.\n Experience\nOmdena Nov 2023 \u2013 Present\nJunior Machine Learning Engineer Remote\n\u2022Collaborated with 25+ AI Engineers, developers and clients in remote settings, ensuring seamless project execution\nand delivery of AI applications or products which solves real word problems.\n \u2022Done web scraping using beautiful soup and Apify to collect 100,000 data from di\ufb00erent sources to create dataset\nfor quality prediction.\n \u2022Used Hugging face models like facebook/bart-large-mnli (zero-shot classi\ufb01cation ) and used language translation\nmodels like T-5,Bert and Deberta llm.\n \u2022Implemented gtts and speech recognition models like whisper for better user experience and interaction.\n", "original_text": "Experience\nOmdena Nov 2023 \u2013 Present\nJunior Machine Learning Engineer Remote\n\u2022Collaborated with 25+ AI Engineers, developers and clients in remote settings, ensuring seamless project execution\nand delivery of AI applications or products which solves real word problems.\n"}, "hash": "f31a5d3be96829b90ddae0d048b3e33f16fa21b0e9077f8dc4ffb9718ebb7de1", "class_name": "RelatedNodeInfo"}}, "text": "\u2022Conducted comparative analyses of accident statistics across regions and time periods, facilitating the assessment of\nsafety measures and interventions.\n", "start_char_idx": 4110, "end_char_idx": 4264, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3cf3ca8e-3b4c-4beb-9ff2-e80403c73a90": {"__data__": {"id_": "3cf3ca8e-3b4c-4beb-9ff2-e80403c73a90", "embedding": null, "metadata": {"window": "Road Accident Dashboard |Tableau Link\n\u2022Developed a comprehensive road accident dashboard using Tableau to analyze 2019 - 2022 accident data and\nimprove decision-making processes.\n \u2022Categorized accidents by 3 levels of severity and used visualizations to delve into the factors contributing to severe\naccidents.\n \u2022Conducted comparative analyses of accident statistics across regions and time periods, facilitating the assessment of\nsafety measures and interventions.\n Experience\nOmdena Nov 2023 \u2013 Present\nJunior Machine Learning Engineer Remote\n\u2022Collaborated with 25+ AI Engineers, developers and clients in remote settings, ensuring seamless project execution\nand delivery of AI applications or products which solves real word problems.\n \u2022Done web scraping using beautiful soup and Apify to collect 100,000 data from di\ufb00erent sources to create dataset\nfor quality prediction.\n \u2022Used Hugging face models like facebook/bart-large-mnli (zero-shot classi\ufb01cation ) and used language translation\nmodels like T-5,Bert and Deberta llm.\n \u2022Implemented gtts and speech recognition models like whisper for better user experience and interaction.\n", "original_text": "Experience\nOmdena Nov 2023 \u2013 Present\nJunior Machine Learning Engineer Remote\n\u2022Collaborated with 25+ AI Engineers, developers and clients in remote settings, ensuring seamless project execution\nand delivery of AI applications or products which solves real word problems.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "82d84f09-7d30-435f-839f-bf83e19b907b", "node_type": "4", "metadata": {}, "hash": "b75d4ee1f34ec4f633bd750cdc013c8465514c53ed5e8a2c488c15f028158ec9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "da7b9481-f7d5-4857-a91b-963cc0480ddf", "node_type": "1", "metadata": {"window": "\u2022Model \ufb01ne-tuned on Mental Health Dataset using Qlora and Peft.\n Road Accident Dashboard |Tableau Link\n\u2022Developed a comprehensive road accident dashboard using Tableau to analyze 2019 - 2022 accident data and\nimprove decision-making processes.\n \u2022Categorized accidents by 3 levels of severity and used visualizations to delve into the factors contributing to severe\naccidents.\n \u2022Conducted comparative analyses of accident statistics across regions and time periods, facilitating the assessment of\nsafety measures and interventions.\n Experience\nOmdena Nov 2023 \u2013 Present\nJunior Machine Learning Engineer Remote\n\u2022Collaborated with 25+ AI Engineers, developers and clients in remote settings, ensuring seamless project execution\nand delivery of AI applications or products which solves real word problems.\n \u2022Done web scraping using beautiful soup and Apify to collect 100,000 data from di\ufb00erent sources to create dataset\nfor quality prediction.\n \u2022Used Hugging face models like facebook/bart-large-mnli (zero-shot classi\ufb01cation ) and used language translation\nmodels like T-5,Bert and Deberta llm.\n", "original_text": "\u2022Conducted comparative analyses of accident statistics across regions and time periods, facilitating the assessment of\nsafety measures and interventions.\n"}, "hash": "c9e18d0fdb1df70c017195270c0531364730cb99d7927e02baae495f42e97874", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1022e526-1a26-4c14-96c4-e2c296117622", "node_type": "1", "metadata": {"window": "\u2022Categorized accidents by 3 levels of severity and used visualizations to delve into the factors contributing to severe\naccidents.\n \u2022Conducted comparative analyses of accident statistics across regions and time periods, facilitating the assessment of\nsafety measures and interventions.\n Experience\nOmdena Nov 2023 \u2013 Present\nJunior Machine Learning Engineer Remote\n\u2022Collaborated with 25+ AI Engineers, developers and clients in remote settings, ensuring seamless project execution\nand delivery of AI applications or products which solves real word problems.\n \u2022Done web scraping using beautiful soup and Apify to collect 100,000 data from di\ufb00erent sources to create dataset\nfor quality prediction.\n \u2022Used Hugging face models like facebook/bart-large-mnli (zero-shot classi\ufb01cation ) and used language translation\nmodels like T-5,Bert and Deberta llm.\n \u2022Implemented gtts and speech recognition models like whisper for better user experience and interaction.\n Education\nData Science using python Dec 2023 - Jan 2024\nBrototype Ernakulam, Kerala\nBig data data Science using python and AWS Jan 2022 - Aug 2022\nLuminar Technolab Ernakulam, Kerala\nB.com with Logistics and Supply Chain , CGPA : 6 2018 - 2021\nKristu Jayanti College Bangalore, Karnataka", "original_text": "\u2022Done web scraping using beautiful soup and Apify to collect 100,000 data from di\ufb00erent sources to create dataset\nfor quality prediction.\n"}, "hash": "9f4aa4be89cf852809bf7818bd402794ca962b3fb8607ac0cda5d3f3195d4061", "class_name": "RelatedNodeInfo"}}, "text": "Experience\nOmdena Nov 2023 \u2013 Present\nJunior Machine Learning Engineer Remote\n\u2022Collaborated with 25+ AI Engineers, developers and clients in remote settings, ensuring seamless project execution\nand delivery of AI applications or products which solves real word problems.\n", "start_char_idx": 4264, "end_char_idx": 4534, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1022e526-1a26-4c14-96c4-e2c296117622": {"__data__": {"id_": "1022e526-1a26-4c14-96c4-e2c296117622", "embedding": null, "metadata": {"window": "\u2022Categorized accidents by 3 levels of severity and used visualizations to delve into the factors contributing to severe\naccidents.\n \u2022Conducted comparative analyses of accident statistics across regions and time periods, facilitating the assessment of\nsafety measures and interventions.\n Experience\nOmdena Nov 2023 \u2013 Present\nJunior Machine Learning Engineer Remote\n\u2022Collaborated with 25+ AI Engineers, developers and clients in remote settings, ensuring seamless project execution\nand delivery of AI applications or products which solves real word problems.\n \u2022Done web scraping using beautiful soup and Apify to collect 100,000 data from di\ufb00erent sources to create dataset\nfor quality prediction.\n \u2022Used Hugging face models like facebook/bart-large-mnli (zero-shot classi\ufb01cation ) and used language translation\nmodels like T-5,Bert and Deberta llm.\n \u2022Implemented gtts and speech recognition models like whisper for better user experience and interaction.\n Education\nData Science using python Dec 2023 - Jan 2024\nBrototype Ernakulam, Kerala\nBig data data Science using python and AWS Jan 2022 - Aug 2022\nLuminar Technolab Ernakulam, Kerala\nB.com with Logistics and Supply Chain , CGPA : 6 2018 - 2021\nKristu Jayanti College Bangalore, Karnataka", "original_text": "\u2022Done web scraping using beautiful soup and Apify to collect 100,000 data from di\ufb00erent sources to create dataset\nfor quality prediction.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "82d84f09-7d30-435f-839f-bf83e19b907b", "node_type": "4", "metadata": {}, "hash": "b75d4ee1f34ec4f633bd750cdc013c8465514c53ed5e8a2c488c15f028158ec9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3cf3ca8e-3b4c-4beb-9ff2-e80403c73a90", "node_type": "1", "metadata": {"window": "Road Accident Dashboard |Tableau Link\n\u2022Developed a comprehensive road accident dashboard using Tableau to analyze 2019 - 2022 accident data and\nimprove decision-making processes.\n \u2022Categorized accidents by 3 levels of severity and used visualizations to delve into the factors contributing to severe\naccidents.\n \u2022Conducted comparative analyses of accident statistics across regions and time periods, facilitating the assessment of\nsafety measures and interventions.\n Experience\nOmdena Nov 2023 \u2013 Present\nJunior Machine Learning Engineer Remote\n\u2022Collaborated with 25+ AI Engineers, developers and clients in remote settings, ensuring seamless project execution\nand delivery of AI applications or products which solves real word problems.\n \u2022Done web scraping using beautiful soup and Apify to collect 100,000 data from di\ufb00erent sources to create dataset\nfor quality prediction.\n \u2022Used Hugging face models like facebook/bart-large-mnli (zero-shot classi\ufb01cation ) and used language translation\nmodels like T-5,Bert and Deberta llm.\n \u2022Implemented gtts and speech recognition models like whisper for better user experience and interaction.\n", "original_text": "Experience\nOmdena Nov 2023 \u2013 Present\nJunior Machine Learning Engineer Remote\n\u2022Collaborated with 25+ AI Engineers, developers and clients in remote settings, ensuring seamless project execution\nand delivery of AI applications or products which solves real word problems.\n"}, "hash": "f31a5d3be96829b90ddae0d048b3e33f16fa21b0e9077f8dc4ffb9718ebb7de1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c442f7c4-8d42-44e4-b7ff-34afbbbb6253", "node_type": "1", "metadata": {"window": "\u2022Conducted comparative analyses of accident statistics across regions and time periods, facilitating the assessment of\nsafety measures and interventions.\n Experience\nOmdena Nov 2023 \u2013 Present\nJunior Machine Learning Engineer Remote\n\u2022Collaborated with 25+ AI Engineers, developers and clients in remote settings, ensuring seamless project execution\nand delivery of AI applications or products which solves real word problems.\n \u2022Done web scraping using beautiful soup and Apify to collect 100,000 data from di\ufb00erent sources to create dataset\nfor quality prediction.\n \u2022Used Hugging face models like facebook/bart-large-mnli (zero-shot classi\ufb01cation ) and used language translation\nmodels like T-5,Bert and Deberta llm.\n \u2022Implemented gtts and speech recognition models like whisper for better user experience and interaction.\n Education\nData Science using python Dec 2023 - Jan 2024\nBrototype Ernakulam, Kerala\nBig data data Science using python and AWS Jan 2022 - Aug 2022\nLuminar Technolab Ernakulam, Kerala\nB.com with Logistics and Supply Chain , CGPA : 6 2018 - 2021\nKristu Jayanti College Bangalore, Karnataka", "original_text": "\u2022Used Hugging face models like facebook/bart-large-mnli (zero-shot classi\ufb01cation ) and used language translation\nmodels like T-5,Bert and Deberta llm.\n"}, "hash": "1e6a4bed6b857f626c77be1c22ab868ee09944fc33ef2e7ccf5806f21ba75764", "class_name": "RelatedNodeInfo"}}, "text": "\u2022Done web scraping using beautiful soup and Apify to collect 100,000 data from di\ufb00erent sources to create dataset\nfor quality prediction.\n", "start_char_idx": 4534, "end_char_idx": 4672, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c442f7c4-8d42-44e4-b7ff-34afbbbb6253": {"__data__": {"id_": "c442f7c4-8d42-44e4-b7ff-34afbbbb6253", "embedding": null, "metadata": {"window": "\u2022Conducted comparative analyses of accident statistics across regions and time periods, facilitating the assessment of\nsafety measures and interventions.\n Experience\nOmdena Nov 2023 \u2013 Present\nJunior Machine Learning Engineer Remote\n\u2022Collaborated with 25+ AI Engineers, developers and clients in remote settings, ensuring seamless project execution\nand delivery of AI applications or products which solves real word problems.\n \u2022Done web scraping using beautiful soup and Apify to collect 100,000 data from di\ufb00erent sources to create dataset\nfor quality prediction.\n \u2022Used Hugging face models like facebook/bart-large-mnli (zero-shot classi\ufb01cation ) and used language translation\nmodels like T-5,Bert and Deberta llm.\n \u2022Implemented gtts and speech recognition models like whisper for better user experience and interaction.\n Education\nData Science using python Dec 2023 - Jan 2024\nBrototype Ernakulam, Kerala\nBig data data Science using python and AWS Jan 2022 - Aug 2022\nLuminar Technolab Ernakulam, Kerala\nB.com with Logistics and Supply Chain , CGPA : 6 2018 - 2021\nKristu Jayanti College Bangalore, Karnataka", "original_text": "\u2022Used Hugging face models like facebook/bart-large-mnli (zero-shot classi\ufb01cation ) and used language translation\nmodels like T-5,Bert and Deberta llm.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "82d84f09-7d30-435f-839f-bf83e19b907b", "node_type": "4", "metadata": {}, "hash": "b75d4ee1f34ec4f633bd750cdc013c8465514c53ed5e8a2c488c15f028158ec9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1022e526-1a26-4c14-96c4-e2c296117622", "node_type": "1", "metadata": {"window": "\u2022Categorized accidents by 3 levels of severity and used visualizations to delve into the factors contributing to severe\naccidents.\n \u2022Conducted comparative analyses of accident statistics across regions and time periods, facilitating the assessment of\nsafety measures and interventions.\n Experience\nOmdena Nov 2023 \u2013 Present\nJunior Machine Learning Engineer Remote\n\u2022Collaborated with 25+ AI Engineers, developers and clients in remote settings, ensuring seamless project execution\nand delivery of AI applications or products which solves real word problems.\n \u2022Done web scraping using beautiful soup and Apify to collect 100,000 data from di\ufb00erent sources to create dataset\nfor quality prediction.\n \u2022Used Hugging face models like facebook/bart-large-mnli (zero-shot classi\ufb01cation ) and used language translation\nmodels like T-5,Bert and Deberta llm.\n \u2022Implemented gtts and speech recognition models like whisper for better user experience and interaction.\n Education\nData Science using python Dec 2023 - Jan 2024\nBrototype Ernakulam, Kerala\nBig data data Science using python and AWS Jan 2022 - Aug 2022\nLuminar Technolab Ernakulam, Kerala\nB.com with Logistics and Supply Chain , CGPA : 6 2018 - 2021\nKristu Jayanti College Bangalore, Karnataka", "original_text": "\u2022Done web scraping using beautiful soup and Apify to collect 100,000 data from di\ufb00erent sources to create dataset\nfor quality prediction.\n"}, "hash": "9f4aa4be89cf852809bf7818bd402794ca962b3fb8607ac0cda5d3f3195d4061", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "69f6d15b-04f8-47fb-8707-2c20e0ae380b", "node_type": "1", "metadata": {"window": "Experience\nOmdena Nov 2023 \u2013 Present\nJunior Machine Learning Engineer Remote\n\u2022Collaborated with 25+ AI Engineers, developers and clients in remote settings, ensuring seamless project execution\nand delivery of AI applications or products which solves real word problems.\n \u2022Done web scraping using beautiful soup and Apify to collect 100,000 data from di\ufb00erent sources to create dataset\nfor quality prediction.\n \u2022Used Hugging face models like facebook/bart-large-mnli (zero-shot classi\ufb01cation ) and used language translation\nmodels like T-5,Bert and Deberta llm.\n \u2022Implemented gtts and speech recognition models like whisper for better user experience and interaction.\n Education\nData Science using python Dec 2023 - Jan 2024\nBrototype Ernakulam, Kerala\nBig data data Science using python and AWS Jan 2022 - Aug 2022\nLuminar Technolab Ernakulam, Kerala\nB.com with Logistics and Supply Chain , CGPA : 6 2018 - 2021\nKristu Jayanti College Bangalore, Karnataka", "original_text": "\u2022Implemented gtts and speech recognition models like whisper for better user experience and interaction.\n"}, "hash": "2fef1c37d2c5520b161b4d117a8e4938f701381f9ce36359b043a7ff425cfec9", "class_name": "RelatedNodeInfo"}}, "text": "\u2022Used Hugging face models like facebook/bart-large-mnli (zero-shot classi\ufb01cation ) and used language translation\nmodels like T-5,Bert and Deberta llm.\n", "start_char_idx": 4672, "end_char_idx": 4823, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "69f6d15b-04f8-47fb-8707-2c20e0ae380b": {"__data__": {"id_": "69f6d15b-04f8-47fb-8707-2c20e0ae380b", "embedding": null, "metadata": {"window": "Experience\nOmdena Nov 2023 \u2013 Present\nJunior Machine Learning Engineer Remote\n\u2022Collaborated with 25+ AI Engineers, developers and clients in remote settings, ensuring seamless project execution\nand delivery of AI applications or products which solves real word problems.\n \u2022Done web scraping using beautiful soup and Apify to collect 100,000 data from di\ufb00erent sources to create dataset\nfor quality prediction.\n \u2022Used Hugging face models like facebook/bart-large-mnli (zero-shot classi\ufb01cation ) and used language translation\nmodels like T-5,Bert and Deberta llm.\n \u2022Implemented gtts and speech recognition models like whisper for better user experience and interaction.\n Education\nData Science using python Dec 2023 - Jan 2024\nBrototype Ernakulam, Kerala\nBig data data Science using python and AWS Jan 2022 - Aug 2022\nLuminar Technolab Ernakulam, Kerala\nB.com with Logistics and Supply Chain , CGPA : 6 2018 - 2021\nKristu Jayanti College Bangalore, Karnataka", "original_text": "\u2022Implemented gtts and speech recognition models like whisper for better user experience and interaction.\n"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "82d84f09-7d30-435f-839f-bf83e19b907b", "node_type": "4", "metadata": {}, "hash": "b75d4ee1f34ec4f633bd750cdc013c8465514c53ed5e8a2c488c15f028158ec9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c442f7c4-8d42-44e4-b7ff-34afbbbb6253", "node_type": "1", "metadata": {"window": "\u2022Conducted comparative analyses of accident statistics across regions and time periods, facilitating the assessment of\nsafety measures and interventions.\n Experience\nOmdena Nov 2023 \u2013 Present\nJunior Machine Learning Engineer Remote\n\u2022Collaborated with 25+ AI Engineers, developers and clients in remote settings, ensuring seamless project execution\nand delivery of AI applications or products which solves real word problems.\n \u2022Done web scraping using beautiful soup and Apify to collect 100,000 data from di\ufb00erent sources to create dataset\nfor quality prediction.\n \u2022Used Hugging face models like facebook/bart-large-mnli (zero-shot classi\ufb01cation ) and used language translation\nmodels like T-5,Bert and Deberta llm.\n \u2022Implemented gtts and speech recognition models like whisper for better user experience and interaction.\n Education\nData Science using python Dec 2023 - Jan 2024\nBrototype Ernakulam, Kerala\nBig data data Science using python and AWS Jan 2022 - Aug 2022\nLuminar Technolab Ernakulam, Kerala\nB.com with Logistics and Supply Chain , CGPA : 6 2018 - 2021\nKristu Jayanti College Bangalore, Karnataka", "original_text": "\u2022Used Hugging face models like facebook/bart-large-mnli (zero-shot classi\ufb01cation ) and used language translation\nmodels like T-5,Bert and Deberta llm.\n"}, "hash": "1e6a4bed6b857f626c77be1c22ab868ee09944fc33ef2e7ccf5806f21ba75764", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d597229d-1765-4679-81bc-808fae99d8d7", "node_type": "1", "metadata": {"window": "\u2022Done web scraping using beautiful soup and Apify to collect 100,000 data from di\ufb00erent sources to create dataset\nfor quality prediction.\n \u2022Used Hugging face models like facebook/bart-large-mnli (zero-shot classi\ufb01cation ) and used language translation\nmodels like T-5,Bert and Deberta llm.\n \u2022Implemented gtts and speech recognition models like whisper for better user experience and interaction.\n Education\nData Science using python Dec 2023 - Jan 2024\nBrototype Ernakulam, Kerala\nBig data data Science using python and AWS Jan 2022 - Aug 2022\nLuminar Technolab Ernakulam, Kerala\nB.com with Logistics and Supply Chain , CGPA : 6 2018 - 2021\nKristu Jayanti College Bangalore, Karnataka", "original_text": "Education\nData Science using python Dec 2023 - Jan 2024\nBrototype Ernakulam, Kerala\nBig data data Science using python and AWS Jan 2022 - Aug 2022\nLuminar Technolab Ernakulam, Kerala\nB.com with Logistics and Supply Chain , CGPA : 6 2018 - 2021\nKristu Jayanti College Bangalore, Karnataka"}, "hash": "4ef22799516388a66eba69e46efa3567a7f91d00d6c2ecd71f5a700923321317", "class_name": "RelatedNodeInfo"}}, "text": "\u2022Implemented gtts and speech recognition models like whisper for better user experience and interaction.\n", "start_char_idx": 4823, "end_char_idx": 4928, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d597229d-1765-4679-81bc-808fae99d8d7": {"__data__": {"id_": "d597229d-1765-4679-81bc-808fae99d8d7", "embedding": null, "metadata": {"window": "\u2022Done web scraping using beautiful soup and Apify to collect 100,000 data from di\ufb00erent sources to create dataset\nfor quality prediction.\n \u2022Used Hugging face models like facebook/bart-large-mnli (zero-shot classi\ufb01cation ) and used language translation\nmodels like T-5,Bert and Deberta llm.\n \u2022Implemented gtts and speech recognition models like whisper for better user experience and interaction.\n Education\nData Science using python Dec 2023 - Jan 2024\nBrototype Ernakulam, Kerala\nBig data data Science using python and AWS Jan 2022 - Aug 2022\nLuminar Technolab Ernakulam, Kerala\nB.com with Logistics and Supply Chain , CGPA : 6 2018 - 2021\nKristu Jayanti College Bangalore, Karnataka", "original_text": "Education\nData Science using python Dec 2023 - Jan 2024\nBrototype Ernakulam, Kerala\nBig data data Science using python and AWS Jan 2022 - Aug 2022\nLuminar Technolab Ernakulam, Kerala\nB.com with Logistics and Supply Chain , CGPA : 6 2018 - 2021\nKristu Jayanti College Bangalore, Karnataka"}, "excluded_embed_metadata_keys": ["window", "original_text"], "excluded_llm_metadata_keys": ["window", "original_text"], "relationships": {"1": {"node_id": "82d84f09-7d30-435f-839f-bf83e19b907b", "node_type": "4", "metadata": {}, "hash": "b75d4ee1f34ec4f633bd750cdc013c8465514c53ed5e8a2c488c15f028158ec9", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "69f6d15b-04f8-47fb-8707-2c20e0ae380b", "node_type": "1", "metadata": {"window": "Experience\nOmdena Nov 2023 \u2013 Present\nJunior Machine Learning Engineer Remote\n\u2022Collaborated with 25+ AI Engineers, developers and clients in remote settings, ensuring seamless project execution\nand delivery of AI applications or products which solves real word problems.\n \u2022Done web scraping using beautiful soup and Apify to collect 100,000 data from di\ufb00erent sources to create dataset\nfor quality prediction.\n \u2022Used Hugging face models like facebook/bart-large-mnli (zero-shot classi\ufb01cation ) and used language translation\nmodels like T-5,Bert and Deberta llm.\n \u2022Implemented gtts and speech recognition models like whisper for better user experience and interaction.\n Education\nData Science using python Dec 2023 - Jan 2024\nBrototype Ernakulam, Kerala\nBig data data Science using python and AWS Jan 2022 - Aug 2022\nLuminar Technolab Ernakulam, Kerala\nB.com with Logistics and Supply Chain , CGPA : 6 2018 - 2021\nKristu Jayanti College Bangalore, Karnataka", "original_text": "\u2022Implemented gtts and speech recognition models like whisper for better user experience and interaction.\n"}, "hash": "2fef1c37d2c5520b161b4d117a8e4938f701381f9ce36359b043a7ff425cfec9", "class_name": "RelatedNodeInfo"}}, "text": "Education\nData Science using python Dec 2023 - Jan 2024\nBrototype Ernakulam, Kerala\nBig data data Science using python and AWS Jan 2022 - Aug 2022\nLuminar Technolab Ernakulam, Kerala\nB.com with Logistics and Supply Chain , CGPA : 6 2018 - 2021\nKristu Jayanti College Bangalore, Karnataka", "start_char_idx": 4928, "end_char_idx": 5215, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/ref_doc_info": {"82d84f09-7d30-435f-839f-bf83e19b907b": {"node_ids": ["d1941f75-57a9-44d4-a7f6-60725d8b7092", "d89a01ad-5a21-4fa4-8821-d76bcd957d1c", "f8ee740d-cd33-4327-8880-b2e6157ff358", "013b8180-fc65-41e2-a304-a79ad1e6dbbe", "73bc6480-4e8d-4faa-b739-34334eac9e0f", "877f6fa0-bc6d-4f2a-b190-1c3fddff9d25", "d61d989a-cbfa-435a-a750-cfffed889895", "3fa3a56e-1188-41c5-8c58-c43d303f8539", "7216234e-c894-44bc-99d3-5f8db881c192", "807d0e15-d9b4-4d49-b92a-53b51b9f7ba6", "9001860d-70bd-435a-9d19-9be6a541e917", "0afe309c-44d9-40db-a58e-530a002c8b55", "79b1ee48-d786-4146-a49c-c444f5487301", "37a49867-b891-4ec4-9a6f-0c3adb4b379b", "51895717-bb4a-42ad-8db9-7f2438109c5b", "5fa511f5-e4bc-4f0c-ae6d-51ba33671408", "36ddacfe-da6f-4060-b2c5-59200a90ef73", "b0f8925e-9d7a-44de-b385-ce1f0063ddc5", "08123b7d-7164-4e6b-b9d4-d80b73fd7272", "6feb1a13-9b49-4c67-820a-f7e502d49aa1", "4a978267-d802-40cf-98d3-04cfaef37695", "1c7c8928-6622-45f1-b2c4-02d5e5e0999c", "308937ff-6ad0-4cbb-96a9-35b8a962caa0", "da7b9481-f7d5-4857-a91b-963cc0480ddf", "3cf3ca8e-3b4c-4beb-9ff2-e80403c73a90", "1022e526-1a26-4c14-96c4-e2c296117622", "c442f7c4-8d42-44e4-b7ff-34afbbbb6253", "69f6d15b-04f8-47fb-8707-2c20e0ae380b", "d597229d-1765-4679-81bc-808fae99d8d7"], "metadata": {"window": "Josekutty Jose\nMachine Learning Engineer\n\u2642phone+91 9745949352 /envel\u2322pejosekuttyjose08@gmail.com /linkedinLinkedIn /githubGitHub DagsHub\u1f517LeetCode\nPro\ufb01le Summary\nHighly motivated Machine Learning Engineer over 6+ months of experience in Generative AI with a passion for building\nimpactful AI solutions.  Proven ability to design, develop, and deploy machine learning models across various domains.\n Skilled in data preprocessing, feature engineering, model selection, and hyperparameter tuning .  Experience working with\nTensorFlow, PyTorch, and scikit-learn and Expertise in Quantization, Fine -tuning and improving the models performance.\n", "original_text": "Josekutty Jose\nMachine Learning Engineer\n\u2642phone+91 9745949352 /envel\u2322pejosekuttyjose08@gmail.com /linkedinLinkedIn /githubGitHub DagsHub\u1f517LeetCode\nPro\ufb01le Summary\nHighly motivated Machine Learning Engineer over 6+ months of experience in Generative AI with a passion for building\nimpactful AI solutions. "}}}}